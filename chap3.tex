如图~\ref{fig:keypoint_detection_neural_network}(a) 所示，本文将 EfficientViT 引入 YOLOv8-Pose 的骨干网络，以增强模型对全局特征的关注度。这样，当目标截断、光照剧变等因素导致局部特征无法有效利用时，模型能够更多地依赖全局特征，从而更好地适应空间非合作目标的复杂环境。EfficientViT\cite{liu2023efficientvit}是一种针对视觉 Transformer (Vision Transformer, ViT) 的高效架构，以显著降低计算和存储开销。其核心是在网络中引入新的基础模块 EfficientViT 块，如图~\ref{fig:keypoint_detection_neural_network} (b) 所示，每个模块采用三明治式布局（Sandwich Layout）结构，结合级联组注意力（Cascaded Group Attention, CGA）机制，并辅以参数重分配策略，实现网络的轻量化设计。整个 EfficientViT 网络将上述高效模块堆叠成多阶段结构：首先通过重叠式块嵌入将输入图像划分为若干 patch 并嵌入初始令牌序列，然后分三个阶段逐步对特征图进行降采样，同时逐层提高通道维度和注意力头数。每个阶段由若干个 EfficientViT 模块组成（后续阶段的模块数多于前期，以补偿特征图尺寸的降低），在阶段之间插入 EfficientViT 降采样块以减少分辨率并防止信息损失。EfficientViT 在架构上用批归一化（BatchNorm）取代了传统 Transformer 中的层归一化（Layer Norm），并使用 ReLU 激活函数代替 GELU，以便更好地适配高效推理。通过以上设计，EfficientViT 网络在保持或提升准确率的同时，大幅减少了参数量和计算量，兼顾了精度与效率。如图~\ref{fig:keypoint_detection_neural_network} (b) 所示：Sandwich Layout 结构采用了夹层式的 Transformer 结构，以提高内存访问效率并强化通道信息交流。具体来说，在该模块中仅保留一次多头自注意力（Multi-Head Self-Attention,  MHSA）计算，并在其之前和之后分别堆叠多个前馈网络（Feed-Forward Network，FFN）层，相当于用 FFN "夹住"少量注意力层。从数学公式上，可将第 $i$ 个模块的变换表示为：
\begin{equation}\label{eq:sandwich}
	\mathbf{X}_{i+1} = \Phi^F_i\Big(\Phi^A_i\big(\Phi^F_i(\mathbf{X}_i)\big)\Big)\,
\end{equation}

其中 $\Phi^A_i$ 表示第 $i$ 个模块中的自注意力变换，$\Phi^F_i$ 表示前馈网络变换（包含非线性激活和逐元素加法的两层感知机，膨胀比例适当缩小以减小参数冗余），$\mathbf{X}_i$ 和 $\mathbf{X}_{i+1}$ 分别是模块输入和输出特征。与传统 Transformer 块每层都执行一次注意力不同，Sandwich 布局大幅减少了记忆开销高的全局注意力计算次数，将注意力"稀释"到更少的层中。这种设计显著降低了由于多头注意力读写大规模特征图所带来的内存访问延迟，并通过增加 FFN 层数来保持网络的表达能力和通道通信效率。在 Sandwich 结构的作用下，EfficientViT 模块用更多的参数高效的 FFN 计算取代了大部分注意力计算，使网络在内存和计算开销上更为高效。

对于 CGA 机制，在标准视觉 Transformer 的多头自注意力中，不同注意力头往往学到相似的注意力图，存在计算冗余。CGA 针对此问题对注意力计算方式进行重构，大幅提升计算效率和多样性。其核心思想是将输入特征按通道维度均分为 $h$ 个子组，每个子特征组仅供对应的注意力头计算。这样，每个注意力头只关注输入的一部分通道信息，从而避免了以往每个头都对全通道特征重复计算的浪费。形式上，对于第 $i$ 个模块、划分后的第 $j$ 个特征子组 $\mathbf{X}_{ij}$（$j=1,2,\dots,h$），CGA 的计算过程可用数学公式描述为：
\begin{equation}\label{eq:cga1}
	\mathbf{X}^{e}_{i,j} = \text{Attn}\!\Big(\mathbf{X}_{ij}W^{Q}_{ij},\; \mathbf{X}_{ij}W^{K}_{ij},\; \mathbf{X}_{ij}W^{V}_{ij}\Big)
\end{equation}
\begin{equation}
	\mathbf{X}^{e}_{i+1} = \text{Concat}\big[\mathbf{X}^{e}_{i,1}, \mathbf{X}^{e}_{i,2}, \dots, \mathbf{X}^{e}_{i,h}\big] \, W^P_i\
\end{equation}
其中 $\text{Attn}(Q,K,V)$ 表示标准的自注意力计算，$W^Q_{ij},W^K_{ij},W^V_{ij}$ 为第 $i$ 个模块中第 $j$ 个头的查询、键、值投影矩阵，$W^P_i$ 为输出投影矩阵，将所有头的输出特征串联后映射回原始维度。上述分组方案类似于卷积中的组卷积思想：由于每个头只处理$\frac{1}{h}$的通道，其 Q/K/V 投影的参数量和矩阵乘法计算量也相应减少到原来的 $\frac{1}{h}$，从而理论上令注意力模块在参数和算力开销上缩减近 $h$ 倍。单纯分组虽减少了开销，但可能因每个头仅看局部通道而降低网络表示能力。为此，CGA 进一步引入级联机制来弥补信息交流：将前一注意力头的输出逐步传递并融合到下一头的输入中。用公式表示，即对于 $1<j\le h$，令
\begin{equation}\label{eq:cga2}
	\mathbf{X}'_{i,j} = \mathbf{X}_{ij} + \mathbf{X}^{e}_{i,\,j-1}\,
\end{equation}
其中 $\mathbf{X}'_{i,j}$ 表示更新后的第 $j$ 个子特征组，再将其用于计算第 $j$ 个头的注意力（式\eqref{eq:cga1}）。通过这种逐头累加的级联连接，每个后续注意力头在关注自身子特征的同时，额外融合了前一头输出的全局信息，逐步丰富注意力的表示能力。CGA 还在每个头的查询变换之后附加了一层深度卷积以结合局部相关性，从而使注意力计算能够同时捕获全局和局部依赖关系。得益于以上设计，CGA 带来多重性能提升，各头处理不同子空间的特征显著提高了注意力图的多样性，减少了头之间的信息冗余，由于减少了每个头的通道维度，注意力计算的 FLOPs 和参数量下降明显。

传统的视觉 Transformer 采用单阶段固定长度的序列表示，多个 Transformer 编码器层堆叠，每层包含一次MHSA和一次FFN；而 EfficientViT 采用多阶段金字塔结构，特征图分辨率逐级降低、通道逐级增加，各阶段使用高效的 EfficientViT 块替代标准 Transformer 层。每个 EfficientViT 块仅含一层注意力（相比 ViT 每层都有）却包含双倍数量或以上的 FFN 变换，并通过 DWConv 引入卷积特征，这使得每层的特征提取能力得到了保证。

\subsection{TANeck}
细粒度特征在关键点检测中具有重要作用，尤其针对如图~\ref{fig:keypoint_detection_neural_network}(a) 所示纹理特征不明显的空间非合作目标。这类目标的关键点检测难度较大。因此，为增强 YOLOv8 的 Neck 模块对该类目标的特征提取能力，本文将 Triplet Attention \cite{Misra_2021_WACV} 机制集成其中，构建了 TANeck。Triplet Attention 是一种 CNN 特征域的再加权注意力机制，对细粒度特征具有更强的关注能力。TANeck 的基础框架采用了路径聚合网络，这不同于传统仅自上而下的FPN结构\cite{fpn}。PANet 通过自底向上和自顶向下的双向特征传递机制丰富了特征层次结构。该双向增强方法有效保留了高分辨率细节，对于识别空间目标的复杂特征至关重要。同时，PANet 架构在解决传统单向 FPN 中常见的语义差距问题的同时，确保深层高层次上下文信息能够更早且更高效地融合。在非合作空间目标的检测任务中，这一点尤为关键，因为关键点的精确定位直接决定了检测性能。

Triplet Attention 机制如图~\ref{fig:keypoint_detection_neural_network} (c) 所示，由三个并行分支组成，可对输入特征图 $\mathbf{X} \in \mathbb{R}^{C \times H \times W}$ 进行跨维度的注意力建模。其核心思想是在计算开销几乎不增加的情况下，实现高效的跨维度特征交互。

第一个分支用于建模通道与宽度维度之间的交互。具体来说，首先交换输入张量的维度，得到 $\mathbf{X}_{1} \in \mathbb{R}^{W \times C \times H}$。然后，对 $\mathbf{X}_{1}$ 执行 Z-Pool 操作，并通过卷积提取通道-宽度维度的交互信息，最后经由 Sigmoid 激活函数获得注意力图：
\begin{equation} 
	\mathbf{M}_{1} = \sigma\!\Big(f_{1}\big(\text{Z-Pool}(\mathbf{X}_{1})\big)\Big)\,
\end{equation}
其中，$\sigma$ 表示 Sigmoid 激活函数，$f_{1}$ 表示第一个分支中的卷积操作，$\text{Z-Pool}$ 表示结合最大池化和平均池化的操作。

类似地，第二个分支关注通道与高度维度之间的交互。通过重新排列输入张量得到 $\mathbf{X}_{2} \in \mathbb{R}^{H \times C \times W}$，并对其执行与第一个分支相同的操作以获得注意力图：
\begin{equation} 
	\mathbf{M}_{2} = \sigma\!\Big(f_{2}\big(\text{Z-Pool}(\mathbf{X}_{2})\big)\Big)
\end{equation}

第三个分支直接作用于原始输入张量 $\mathbf{X}$，采用类似 BAM 的常规注意力机制得到注意力图：
\begin{equation} 
	\mathbf{M}_{3} = \sigma\!\Big(f_{3}\big(\text{Z-Pool}(\mathbf{X})\big)\Big)
\end{equation}

上述三个分支采用相同的注意力计算机制，但分别专注于不同维度间的交互。$\text{Z-Pool}$ 操作通过在通道维度上执行最大池化和平均池化，对该维度进行压缩，其计算方式如下：
\begin{equation} 
	\text{Z-Pool}(\mathbf{X}) = \text{Concat}\Big[\text{MaxPool}_{\text{Channel}}(\mathbf{X}),\ \text{AvgPool}_{\text{Channel}}(\mathbf{X})\Big]\,
\end{equation}
其中，$\text{Concat}$ 表示在通道维度上的拼接。最大池化和平均池化操作分别定义如下：
\begin{equation} 
	\text{MaxPool}_{\text{Channel}}(\mathbf{X}) = \max_{1 \le c \le C} \mathbf{X}_{c,:,:}
\end{equation}
\begin{equation} 
	\text{AvgPool}_{\text{Channel}}(\mathbf{X}) = \frac{1}{C} \sum_{c=1}^{C} \mathbf{X}_{c,:,:}
\end{equation}

接下来，将上述三个分支得到的注意力图逐元素相乘，融合生成最终的注意力图：
\begin{equation} 
	\mathbf{M} = \mathbf{M}_{1} \otimes \mathbf{M}_{2} \otimes \mathbf{M}_{3}\,
\end{equation}
其中，$\otimes$ 表示逐元素相乘。然后利用该融合后的注意力图对原始特征进行重新加权：
\begin{equation} 
	\mathbf{Y} = \mathbf{X} \odot \mathbf{M}\,
\end{equation}
其中，$\odot$ 同样表示逐元素相乘。Triplet Attention 机制在未显著增加计算复杂度的前提下，实现了跨维度特征的高效交互，显著增强了 Neck 的特征提取能力，并已将该模块成功集成至 YOLOv8-pose 框架中。



\subsection{A3DKS 损失}


YOLO-Pose\cite{yolo_pose} 将 YOLOv5 架构扩展到人体姿态估计领域，通过检测人体关键点实现 \citep{yolo_pose}。该方法引入了用于关键点拟合的对象关键点的损失函数，其公式为：
\begin{equation}
	\mathrm{OKS} = \frac{\sum\limits_{i=1}^{N} \exp\left(-\frac{d_i^2}{2\, s^2 \, k_i^2}\right) \cdot v_i}
	{\sum_{i=1}^{N} v_i}
\end{equation}
其中：
\begin{itemize}
	\item $N$ 表示关键点的总数。
	\item $x_i$, $y_i$ 为预测的第 $i$ 个关键点的坐标。
	\item $x_i^{gt}$, $y_i^{gt}$ 为第 $i$ 个关键点的真实坐标。
	\item $d_i^2 = (x_i - x_i^{gt})^2 + (y_i - y_i^{gt})^2$ 是预测关键点与真实关键点之间的欧氏距离的平方。
	\item $k_i$ 是与第 $i$ 个关键点对应的标准差常数，反映了该关键点标注的不确定性。
	\item $s$ 是尺度因子，通常与目标的尺寸相关（定义为目标边框面积的平方根）。
	\item $v_i$ 表示关键点的可见性，当第 $v_i=0$ 时表示关键点不可见，即位于图像范围外。当$v_i=1$时，表示关键点可见，即位于图像范围内。 
\end{itemize}
对应的 OKS 损失定义即为如下
\begin{equation}
	\mathcal{L}_{\text{OKS}} = 1 - \mathrm{OKS}
\end{equation}
针对空间目标关键点检测中的特殊问题，本文提出了 近似 3D 尺度损失 A3DKS 损失函数。该损失函数结合了小孔相机模型。如图\ref{fig:pin_hole_model}所示，该示意图描述了位于3维空间中深度为 Z 的点$P$以$O$为小孔按照焦距 f 投影到相平面上。由于相似三角形，线段$pc$与$PC$成比例，这个比例因子就是$\frac{Z}{f}$。按照这个原理。可以将关键点的深度信息纳入损失计算中，更准确地反映三维空间中的误差。这样得到的 A3DKS 损失函数的公式为：
\begin{equation}
	L_{kpts} = 1 - \frac{\sum\limits_{i=1}^{N} \exp \left( -\left( \frac{d_i Z_i}{f} \right)^2 \cdot \frac{K}{2 s_h^2 k_i^2} \right) v_i}{\sum\limits_{i=1}^{N_{kpts}} v_i}
\end{equation}
其中：
\begin{itemize}
	\item $d_i = \sqrt{(x_i - x_i^{gt})^2 + (y_i - y_i^{gt})^2}$，为第 $i$ 个关键点像素坐标的误差。
	\item $Z_i$：第 $i$ 个关键点的深度值。
	\item $f$：相机的焦距。
	\item $K$：常数系数，用于调整损失的尺度。
	\item $s_h$：所有样本目标面积的调和平均，用于整体损失值缩放到合理区间。
	\item 其他符号同前。
\end{itemize}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{Img/pin_hole_model.pdf}
	\caption{小孔相机模型示意图}
	\label{fig:pin_hole_model}
\end{figure}
该损失函数通过引入深度信息 $Z_n$，有效地将二维位移误差映射到三维空间，提高了损失对不同深度关键点的敏感度，增强了网络对空间目标的检测精度。

\section{数据集的预处理与制作}
为了验证本章算法的性能，这里选用了公开数据集 SPEED+ 与自建的数据集 Starlink 和 Nauka MLM。

\subsection{SPEED+ 数据集简介}
SPEED+ 数据集\cite{speed+}是欧空局在以 PRISMA 任务中的 Tango 卫星(图~\ref{fig:tango})为对象的 6D 姿态估计挑战赛中提出的数据集，它是在上一代 SPEED\cite{SPEED-Dataset}数据集上对数据集的样本进行了扩充和丰富。其中包含了六万张左右的合 Tango 卫星的合成图像。该数据集为深度学习网络提供了可大规模获取的合成训练样本和相应的高精度姿态标注。SPEED+ 在 SPEED 基础上进一步扩充了合成图像数量和多样性，为算法训练与消融实验提供了更丰富、稳定的样本来源。其数据集中的 Tango 卫星的 6D 姿态图像如图~\ref{fig:tango_demo}所示，数据集重包含了各种有挑战性的情境。图~\ref{fig:tango_demo}(a)为 Tango 卫星位姿图像常规的场景，可以看出 Tango 卫星或远或近，姿态各异。与地面目标不同，空间中的三自由度变换变得更为明显，目标容易以各种朝向对准观测者。图~\ref{fig:tango_demo}(b)为 Tango 卫星位姿图像中目标因部分超出视野而被截断的场景，这导致了部分特征的不可见。图~\ref{fig:tango_demo}(c)为 Tango 卫星出现在地球这个大背景下的场景。相对于纯黑色的深空，这种场景的区分度不是那么明显，且 Tango 的距离过远也容易混入背景导致难以识别。图~\ref{fig:tango_demo}(d)为光照过暗目标不清晰的场景，这种情况下 Tango 的特征被弱化，使其识别会存在困难。这些情境尤其是(b)(c)(d)会对关键点检测任务造成挑战。也正是因为这个数据集存在着这些挑战，也才能充分验证本章网络对这些情况的鲁棒性，因此本章选用了这着两个数据集来训练和测试本章的关键点检测网络。通过以上的消融实验来证明相应模块或者损失函数改进的有效性。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{Img/tango.jpg}
	\caption{PRISMA任务中的Tango}
	\label{fig:tango}
	\vspace{-12pt}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{Img/tango_demo.png}
	\caption{SPEED+数据集的各种场景 (a)常规场景；(b)目标截断场景；(c)背景干扰场景: (d)光照过暗场景}
	\label{fig:tango_demo}
	\vspace{-3pt}
\end{figure}

\subsection{SPEED+ 数据集关键点的标注}
由于 SPEED+ 数据集缺乏空间非合作目标的关键点标注，因此这里将根据一定的特征手动选择关键点，并采用多视图几何的方法测算出关键点的 3D 坐标，从而能够进一步通过数据集的 6D 姿态标注和相机内参计算出关键点的像素坐标。

首先进行关键点 3D 坐标的选取与计算操作。为了使得关键点在图像中更容易被检测出，本章选取了 Tango 卫星的明显角点（如天线顶点，主体框架的角点）为关键点。接下来针对选取的关键点进行逐一的关键点 3D 坐标的计算。其基本操作是通过少量图像关键点像素坐标的手工标注，通过投影约束建立方程组计算出关键点的 3D 坐标。一旦有了关键点 3D 坐标就能通过投影快速生成所有图像关键点的像素坐标。
设在空间非合作目标上选定的某个关键点，其真实 3D 坐标（在目标自身坐标系或某一公共坐标系下）为
\begin{equation}
	\mathbf{P}_a = (X_a,\; Y_a,\; Z_a,\; 1)
\end{equation}
在某张2D图像（索引记为 $ab$）中，对应的像素坐标手工标记值记作
\begin{equation}
	\boldsymbol{\omega}_{ab} = \bigl(x_{ab},\; y_{ab}\bigr)
\end{equation}
根据数据集的6D姿态标注可知：目标与相机之间的相对姿态可由旋转 $R_{ab}$ 和平移 $\mathbf{t}_{ab}$ 来描述；结合相机内参矩阵 $K$，便可构造投影矩阵
\begin{equation}\label{eq:proj_ab_def}
	\mathrm{Proj}_{ab} \;=\; K\,\bigl[\,R_{ab}\;\big\vert\;\mathbf{t}_{ab}\bigr]
\end{equation}
其中 $R_{ab}\in\mathbb{R}^{3\times 3}$，$\mathbf{t}_{ab}\in\mathbb{R}^{3\times 1}$，$K$ 是已知的相机内参矩阵（3$\times$3）。
3D 点 $\mathbf{P}_a$ 与其在 2D 图像上的投影 $\mathbf{P}_{ab} = (x_{ab},\,y_{ab},\,1)$ 应有关系：
\begin{equation}
	\mathbf{P}_{ab}
	\;\sim\;
	\mathrm{Proj}_{ab}\,\mathbf{P}_a
	\;=\;
	K\,[\,R_{ab}\mid\mathbf{t}_{ab}\,]
	\begin{bmatrix}
		X_a\\[4pt] Y_a\\[4pt] Z_a\\[4pt] 1
	\end{bmatrix}
\end{equation}
其中 ``$\sim$'' 表示二者仅相差一个非零尺度因子（因为图像坐标用齐次坐标表示时，$(x,\,y,\,1)$ 与 $\lambda(x,\,y,\,1)$ 表示同一个像素点）。为将其转化为可线性求解的形式，通常对下述两者做叉乘或分列，得到线性方程。将
\begin{equation}
	\bigl(x_{ab},\;y_{ab},\;1\bigr)^\mathrm{T}
	\quad\text{与}\quad
	\mathrm{Proj}_{ab}\,\mathbf{P}_a
\end{equation}
做叉乘可得：
\begin{equation}
	\bigl(x_{ab}\,\mathrm{Proj}_{ab3} \;-\; \mathrm{Proj}_{ab1}\bigr)\,
	\bigl[X_a,\;Y_a,\;Z_a,\;1\bigr]^\mathrm{T} \;=\; 0
\end{equation}
\begin{equation}
	\bigl(y_{ab}\,\mathrm{Proj}_{ab3} \;-\; \mathrm{Proj}_{ab2}\bigr)\,
	\bigl[X_a,\;Y_a,\;Z_a,\;1\bigr]^\mathrm{T} \;=\; 0
\end{equation}
其中，$\mathrm{Proj}_{ab1}$、$\mathrm{Proj}_{ab2}$、$\mathrm{Proj}_{ab3}$ 分别表示投影矩阵 $\mathrm{Proj}_{ab}$ 的第 1、2、3 列。

若某关键点在 $g$ 幅图像中均可观测，则可从每幅图像中各获得两条线性约束，合计 $2g$ 条，将它们堆叠后记作
\begin{equation}\label{eq:Aeq0}
	A
	\begin{bmatrix}
		X_a\\[3pt] Y_a\\[3pt] Z_a\\[3pt] 1
	\end{bmatrix}
	= 0
\end{equation}
其中，$A \in \mathbb{R}^{2g\times 4}$，每两行对应一幅图像的投影方程。为确定 $(X_a,\,Y_a,\,Z_a,\,1)$，需要至少 $g \ge 2$ 幅图像才能获得非平凡解。

由于向量 $\bigl(X_a,\,Y_a,\,Z_a,\,1\bigr)^{\mathrm{T}}$ 处于 $A$ 的零空间，而 $A$ 的秩通常为 3（或接近 3），其零空间是一维空间。利用奇异值分解（SVD）可直接得到该零空间的基向量：
\begin{equation}
	A = U\,\Sigma\,V^\mathrm{T}
\end{equation}
若奇异值按从大到小排列，则 $V$ 的最后一列生成 $A$ 的零空间，记作 $\mathbf{v}_4$。因此
\begin{equation}
	\bigl(X_a,\,Y_a,\,Z_a,\,1\bigr)^\mathrm{T}
	\;\sim\;
	\mathbf{v}_4
\end{equation}
确定关键点的三维坐标 $\mathbf{P}_a$ 后，可基于各图像的投影矩阵 $\mathrm{Proj}_{ab}$
（其显式形式为 $K\,[\,R_{ab}\mid\mathbf{t}_{ab}\,]$）将该点投影到相应图像平面：
\begin{equation}
	\bigl(x_{ab},\,y_{ab},\,1\bigr)^\mathrm{T}
	\;\sim\;
	\mathrm{Proj}_{ab}\,\mathbf{P}_a
\end{equation}
在此基础上，便可在每张图像上自动生成关键点的 2D 像素标注，用于后续的监督或标注文件制作，大幅降低人工标注的工作量并提升了标注精度。通过以上的计算得到的 SPEED+ 数据集中 Tango 卫星的关键点如图~\ref{fig:SPEEDpluskpt}所示。





\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{Img/SPEEDkpt3d.png}
	\caption{SPEED+数据集Tango卫星的关键点}
	\label{fig:SPEEDpluskpt}
	
\end{figure}

\subsection{自建数据集的仿真渲染}

为了验证算法对不同空间非合作目标的泛化能力，本章另外选取了 Nauka MLM 与 Starlink 两种不同几何形状的航天器模型来制作空间非合作目标 6D 姿态数据集。

为了渲染出较高质量的数据集，这里选用了 Blender 引擎的渲染方式
为了制作出 Nauka MLM 与 Starlink 的 6D 姿态数据集，通过 BlenderProc 工具包调用 Blender 引擎对这两款模型进行渲染从而得到其 6D 姿态数据集。Blender 渲染引擎的界面如图~\ref{fig:showblender}所示，关键要素是待渲染物体，灯光与相机。Blender 通过在其坐标系中加载带渲染物体。通过设置灯光参数与相机位置完成初始化设置，然后再通过 6D 姿态设置，即设置物体的位置和姿态。然后通过相机视角捕获图像，进而完成渲染。通过 Blenderproc 的 api 设置渲染参数和带渲染的模型的 6D 姿态，再通过模拟相机采样就可以得到对应位姿的图像。
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{Img/showblender.png}
	\caption{Blender的界面}
	\label{fig:showblender}
\end{figure}



\subsection{渲染参数设置}
在数据集生成之前，首先要对渲染参数进行初始化设置，包括空间环境参数和相机参数。相机内外参如表 \ref{tab:camera-param} 所示，空间环境与光照参数如表 \ref{tab:env-param} 所示。
\begin{table}[hbt]
	\centering
	\caption{相机参数}
	\label{tab:camera-param}
	\zihao{5} % 局部将表格内字号设置为五号
	\begin{tabular}{lc}
		\toprule[1.5pt]
		参数 & 数值 \\
		\midrule[1pt]
		图像分辨率 
		& $1920 \times 1200$ \\
		焦距 ($f_x, f_y$) 
		& $\bigl(3.003e3,\;3.003e3\bigr)$ \\
		主点 ($c_x, c_y$) 
		& $(960,\;600)$ \\
		径向畸变系数 ($k_1, k_2, k_3$) 
		& $\bigl(-2.238e-1,\;5.141e-1,\;-1.312e-1\bigr)$ \\
		切向畸变系数 ($p_1, p_2$) 
		& $\bigl(-6.650e-4,\;-2.140e-4\bigr)$ \\
		相机位置 (X, Y, Z) 
		& $(0,\;0,\;0)$ \\
		相机朝向 (欧拉角XYZ) 
		& $(0^\circ,\; -180^\circ,\; 0^\circ)$ \\
		\bottomrule[1.5pt]
	\end{tabular}
\end{table}


%-------------------------------------------------------------
% 空间环境与光照参数表
\begin{table}[hbt]\centering
	\caption{空间环境与光照参数}
	\label{tab:env-param}
	\zihao{5} % 局部将表格内字号设置为五号
	\begin{tabular}{lc}
		\toprule[1.5pt]
		参数 & 数值 \\
		\midrule[1pt]
		背景颜色(R, G, B) 
		& $(0,\;0,\;0)$ \\
		背景亮度参数 
		& $0.05$ \\
		光源位置 (X, Y, Z)
		& $(0,\;0,\;-100)$ \\
		光源能量参数 
		& $10^7$ \\
		\bottomrule[1.5pt]
	\end{tabular}
\end{table}
\noindent


表 \ref{tab:camera-param} 中展示了渲染时设置的相机内外参。其中分辨率设为 $1920 \times 1200$，像素焦距设为 $3003.413$以模拟适应较远距离成像的需求，主点设在图像中心 $(960, 600)$。畸变系数说明了镜头的径向和切向畸变分布；相机外参则由位置 $(0,0,0)$ 和欧拉角 $(0^\circ,-180^\circ,0^\circ)$ 指定，保证了相机能够以一个合适的角度拍摄空间非合作目标。表 \ref{tab:env-param} 中的空间环境与光照参数包括全局背景颜色与背景亮度参数，以模拟深空环境下的黑色背景和较低的环境光照；同时给出了场景中主要光源的位置和能量，将光源放置在 $Z$ 轴负方向 100 米处；光源能量参数设为$10^7$用以模拟照明。

通过上述参数设置，成功生成了一系列包含目标物体不同姿态和位置的高质量渲染图像数量。这些图像配合对应的 6D 姿态标签，构成了用于训练和评估 6D 位姿估计的关键点检测网络的高质量数据集。生成的数据集效果图如图\ref{fig:rendered_datasets}所示。其中 Nauka MLM 与 Starlink 的数据量都是6万张。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/rendered_datasets.png}
	\caption{BlenderProc渲染出的位姿数据集}
	\label{fig:rendered_datasets}
\end{figure}

\subsection{关键点的 3D 坐标提取}

由于仿真模型是由离散的多边形网格组成，每个网格由边与边围成的面构成，直接可用的关键点往往较为稀疏。为此，首先提取模型的整体网格框架，并将每条网格边的顶点收集得到初始点云。但该初始点云较为稀疏，尤其在局部镂空及曲率较大区域难以捕捉足够密集的形状细节。为解决此问题，可通过手动添加稠密几何表面补全，并将 Catmull-Clark 细分算法应用到原网格上，从而生成更为丰富的顶点集，以便选取关键点。图\ref{fig:model2pointcloud}演示了将仿真模型转化为基础点云的示意过程。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/extract_point_cloud.png}
	\caption{仿真模型转点云示意图}
	\label{fig:model2pointcloud}
\end{figure}


Catmull-Clark 细分算法由 Edwin Catmull 和 Jim Clark 于1978年提出，是计算机图形学中经典的细分曲面方法之一。其基本思想是：在原有网格基础上不断插入新顶点并更新已有顶点坐标，以获得更高分辨率、更平滑的网格。通常对于每个多边形面，会取该面的所有顶点坐标平均值生成一个新"面顶点"；对于每条边，则将其两端顶点坐标及邻接面的面顶点坐标进行平均，生成新的"边顶点"；对原有顶点本身，则结合邻域内的面顶点、相邻边中点以及原坐标，通过加权平均获取更新后的顶点位置，常见的更新公式可写作
\begin{equation}
	P_{\mathrm{new}} = \frac{F + 2R + (n-3)O}{n}
\end{equation}
其中 $F$ 表示该顶点周围所有相邻面的面顶点坐标平均值，$R$ 是相邻边中点坐标的平均值，$O$ 是原顶点坐标，$n$ 为此顶点的邻域面数。通过多次迭代上述插值与更新步骤，原有网格将细化为一个包含大量顶点的平滑网格。

过在局部大面积镂空的区域手动补点，对于局部大量较小的镂空区域，通过将 Catmull-Clark 细分算法应用到初始稀疏网格上，使得整体网格顶点明显增多。将细分后网格的所有顶点导出后，即可得到更密集、更均匀的点云，以避免后续关键点选择的时候，需要选择的关键点实际上在点云中是镂空区域，也为后续第~\ref{chap:SpaceFreeMotionEKF}章的第~\ref{subsection:motion_initial_settings}小节中的计算 Starlink 的转动惯量提供更贴近真实其的几何信息。

\subsection{渲染数据集关键点的标注}

经过以上方法可得到较稠密的点云，再根据特征明显程度以及尽可能均匀分布的原则，分别在Nauka MLM与Starlink的稠密点云上直接选取了17个和12个关键点，如图\ref{fig:rendered_lableled_kpts}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/rendered_lableled_kpts.png}
	\caption{渲染数据集的关键点标注示例}
	\label{fig:rendered_lableled_kpts}
\end{figure}



\section{网络训练}

\subsection{数据增广}
为了预防网络在数据集上发生过拟合，同时也为了提升网络的泛化能力，这里对数据集进行了几何与色彩两种增广。增广效果如图\ref{fig:data_augmentation}所示。
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/data_augmentation.png}
	\caption{数据增广}
	\label{fig:data_augmentation}
\end{figure}

在色彩增广中，本章选择使用 HSV（色调、饱和度、亮度）色彩空间而非 RGB 色彩空间。HSV 色彩空间更符合人类对颜色的感知方式，能够独立调整色调、饱和度和亮度，而不会影响图像的结构信息。这有助于网络学习在不同光照和颜色条件下的鲁棒性。表~\ref{tab:hsv_augmentation} 展示了本文使用的 HSV 色彩增广参数。其中，\(\mathrm{hsv}_h = 0.015\) 仅引入约 \(1.5\%\) 的色调扰动，以保证目标语义不会因颜色漂移而失效；\(\mathrm{hsv}_s = 0.7\) 允许饱和度在原有基础上伸缩至 \([0.3,\,1.3]\)，覆盖从灰暗到鲜艳的大多数真实场景；\(\mathrm{hsv}_v = 0.4\) 则对应亮度在 \(\pm40\%\) 的范围内波动，可有效模拟拍摄中的常见的曝光差异。

\begin{table}[htbp]
	\centering
	\caption{HSV色彩增广参数}
	\label{tab:hsv_augmentation}
	\zihao{5} % 局部将表格内字号设置为五号
	\begin{tabular}{lcp{8cm}}
		\toprule[1.5pt]
		参数 & 数值 & 作用 \\
		\midrule[1pt]
		$\mathrm{hsv}_h$ & 0.015 & 对图像的色调进行微小调整，引入颜色的微小变化 \\
		$\mathrm{hsv}_s$ & 0.7 & 调整图像的饱和度，增强或减弱颜色的纯度 \\
		$\mathrm{hsv}_v$ & 0.4 & 调整图像的亮度，使网络适应不同的亮度条件 \\
		\bottomrule[1.5pt]
	\end{tabular}
\end{table}

在色彩增广的同时叠加几何增广，几何增广通过改变图像的空间结构增强网络对不同姿态和视角的适应性。几何增广的具体方法和参数如表~\ref{tab:geometric_augmentation} 所示。其中，平移幅度被限制在图像尺寸的 \(10\%\) 以内，以避免目标完全移出视野；缩放因子（表中记为 0.5）可使目标在 \(50\%\) 的尺度范围内变化，从而覆盖远近场景；左右翻转以 \(50\%\) 的概率执行，用于消除可能存在的左右视角偏差；马赛克拼接始终开启（概率 1.0），通过将四张样本混合为一张图像增加数据集的样本的多样性，以防过多 epoch 遍历同样样本导致过拟合。

\begin{table}[htbp]
	\centering
	\caption{几何增广方法与参数设置}
	\label{tab:geometric_augmentation}
	\zihao{5} % 局部将表格内字号设置为五号
	\begin{tabular}{lcp{8cm}}
		\toprule[1.5pt]
		增广方法 & 参数值 & 作用描述 \\
		\midrule[1pt]
		平移 & 0.1 & 在水平和垂直方向上平移图像，幅度为图像尺寸的10\%，模拟目标位置的变化 \\
		缩放 & 0.5 & 按比例缩放图像，缩放因子在0.5范围内变化，模拟不同的目标尺度 \\
		左右翻转 & 0.5 & 以50\%的概率对图像进行左右翻转，增加数据的多样性 \\
		马赛克拼接 & 1.0 & 将四张图像拼接成一张，模拟复杂背景和多目标场景 \\
		\bottomrule[1.5pt]
	\end{tabular}
\end{table}

\subsection{优化器设置}
优化器在网络训练中起着关键作用，直接影响网络的收敛速度和效果。考虑到本文研究的数据集的训练样本高达4万多张图像，因此本文选择了 SGD 优化器。SGD 优化器是一种基于随机梯度下降的方法，通过对每个训练样本或小批量样本计算梯度并更新参数，能够较好地避免局部最优问题，并具有良好的泛化能力。同时SGD 的实现简单且计算复杂度低，适用于大规模数据集。在结合合适的学习率调度和动量设置后，SGD 能够实现稳定且高效的网络收敛。因此，综合考虑网络训练的稳定性和泛化性能，本章最终选择了 SGD 优化器。 

\subsection{其他策略}

为了减少训练过程中的震荡加速收敛，这里采用了动量策略。在梯度下降中引入了类似于物理学中的动量概念，其更新公式为：
\begin{equation}
	v_t = \beta v_{t-1} + (1 - \beta) \nabla L(\theta_{t-1})
\end{equation}
\begin{equation}
	\theta_t = \theta_{t-1} - \alpha v_t
\end{equation}
其中，\(v_t\) 是第 \(t\) 次迭代的动量项，\(\beta\) 是动量超参数，\(\nabla L(\theta_{t-1})\) 是损失函数关于参数的梯度，\(\alpha\) 是学习率。动量项帮助优化器在参数更新时保留之前的更新方向，减少震荡，加速收敛。


为了抑制参数过大的变化，增强网络的泛化能力。这里采用了权重衰减，它是一种正则化技术，通过在损失函数中添加 L2 范数惩罚项，防止网络过拟合。其损失函数形式为：
\begin{equation}
	L_{\text{total}} = L_{\text{original}} + \frac{\lambda}{2} \sum_{i} \theta_i^2
\end{equation}
其中，\(L_{\text{original}}\) 为原始损失函数，\(\lambda\) 是权重衰减系数，\(\theta_i\) 为网络参数。权重衰减通过抑制参数的过大增长，增强网络的泛化能力。

学习率是影响网络训练效果的关键超参数。本章采用了学习率预热和学习率衰减策略。学习率预热在训练初期使用较小的学习率，逐步增加到预设的初始学习率，防止训练初期的不稳定。预热策略的公式为：
\begin{equation}
	\alpha_t = \alpha_{\text{init}} \times \frac{t}{T_{\text{warmup}}}
\end{equation}
其中，\(\alpha_t\) 为第 \(t\) 次迭代的学习率，\(\alpha_{\text{init}}\) 为预设的初始学习率（取值为 0.01），\(T_{\text{warmup}}\) 为预热周期（取值为 3 个 epoch）。在训练过程中，逐步降低学习率，有助于网络在收敛后期进行精细的参数调整。最终学习率为初始学习率的 1\%：
\begin{equation}
	\alpha_{\text{final}} = \alpha_{\text{init}} \times \texttt{lrf}
\end{equation}
其中，\(\texttt{lrf}\) 为学习率衰减因子。 

为提升网络的稳定性和泛化能力，这里采用了移动指数平均（Exponential Moving Average, EMA）技术。EMA 对网络参数进行指数加权平均，其更新公式为：
\begin{equation}
	\theta_{\text{EMA}}^t = \beta \theta_{\text{EMA}}^{t-1} + (1 - \beta) \theta^t
\end{equation}
其中，\(\theta_{\text{EMA}}^t\) 为第 \(t\) 次迭代的 EMA 参数，\(\theta^t\) 为当前网络参数，\(\beta\) 为衰减率（通常取值 0.999）。EMA 减少了网络参数的波动，提升了网络在验证集和测试集上的表现。
通过以上的策略组合，可以更有效地保证网络收敛到较高精度。

\subsection{训练参数}
综合上述策略，网络训练的主要参数如表~\ref{tab:train_param}所示。将数据集中47966个样本用于训练网络，11994个样本用于测试网络精度。批量大小 batchsize 设为16，为了使得网络充分收敛，训练周期数 epoch 设为400次，以保证模型尽可能收敛。优化器选择 SGD，这是因为随机梯度下降在处理大规模数据集时内存开销小、计算效率高，且其固有的梯度噪声有助于模型跳出局部极小值、提高泛化能力，与较大的训练周期数配合能够在保证收敛速度的同时有效降低过拟合风险。通过以上设计的训练策略和参数设置，网络能够有效收敛并预防过拟合。

\begin{table}[htbp]
	\centering
	\caption{训练参数}
	\label{tab:train_param}
	{%
		\zihao{5} % 局部将表格内字号设置为五号
		\renewcommand{\arraystretch}{1.2}% 调整行间距
		\begin{tabular}{ccccccccc}
			\toprule[1.5pt]
			设置 & 训练集数量 & 测试集数量 & epoch & batchsize & 优化器\\ 
			\midrule[1pt]
			值 & 47966 & 11994 & 400 & 16 & SGD & \\
			\bottomrule[1.5pt]
		\end{tabular}
	}
\end{table}

\subsection{训练环境}
本实验在表~\ref{tab:training_env}所示的环境中进行训练。使用 Rocky Linux 8.6 操作系统，硬件配置包括 Intel(R) Xeon(R) Platinum 8358P 处理器、NVIDIA A800 GPU和64GB内存。软件环境采用 Python 3.9、PyTorch 2.2.1 和 CUDA 12.1。

\begin{table}[htbp]
	\centering
	\caption{训练环境介绍}
	\label{tab:training_env}
	{%
		\zihao{5} % 局部将表格内字号设置为五号
		\renewcommand{\arraystretch}{1.2}% 调整行间距
		\begin{tabular}{cc}
			\toprule[1.5pt]
			系统配置   & 配置 \\
			\midrule[1pt]
			操作系统   & Rocky Linux 8.6 \\
			CPU        & Intel(R) Xeon(R) Platinum 8358P \\
			GPU        & NVIDIA A800 \\
			内存       & 64GB \\
			Python     & 3.9 \\
			PyTorch    & 2.2 \\
			CUDA       & 12.1 \\
			\bottomrule[1.5pt]
		\end{tabular}
	}
\end{table}



\subsection{损失函数曲线}
本章在以上训练环境，训练参数以及训练测量的基础上在 SPEED+ 数据集, Starlink 数据集以及 Nauka MLM 数据集上进行网络的训练。得到了如图~\ref{fig:loss_cure}的损失函数曲线。可以看出训练开始的几十个 epoch，损失下降迅速，后面逐渐放缓，最后接近400 epoch 的时候，损失下降到了一个较小的值，模型基本收敛，从而得到了训练出的最终权重。
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{Img/loss_curve.png}
	\caption{训练过程的损失函数曲线}
	\label{fig:loss_cure}
	\vspace{-15pt}
\end{figure}
\section{实验结果与分析}
本节首先直接展示了关键点检测网络的关键点检测效果。
为了有效验证相关模块和损失函数改进对于精度提升的有效性，进行一系列的消融实验。由于关键点检测的最终目的是得出目标的 6D 姿态，虽然关键网络检测出的关键点精度越高越好，但是关键点检测的精度与最终 6D 姿态估计的精度对应关系较为复杂。为了更有效地评估关键点检测网络对于最终结果的影响，本节的消融实验将把网络输出的关键点像素坐标通过 PnP 解算出 6D 姿态，在 6D 姿态的评估指标上验证相关模块和损失函数的改进所带来的影响。

\subsection{评估指标}
\label{subsec:est_indicator}
为了有效衡量6D姿态估计的精度，本节实验采用了空间非合作目标6D姿态估计领域常见的指标，该指标由欧空局在SPEED/SPEED+ 数据集上提出，并在大量相关文献中采用。相关的指标定义如下所示。
\begin{itemize}
	\item $err_{\text{t}}$: 真值与预测位置之间的相对误差，表示位置估计的相对精度。该分数无量纲，提供了对位置误差的尺度无关度量。定义如下：
	\begin{equation}
		err_{t,i} = \frac{\| r_{\text{true},i} - r_{\text{pred},i} \|_2}{\| r_{\text{true},i} \|_2}
	\end{equation}
	
	\begin{equation}
		err_t = \frac{1}{N} \sum\limits_{i=1}^{N} err_{t,i}
	\end{equation}
其中 $r_{\text{true},i}$ 和 $r_{\text{pred},i}$ 分别为第 $i$ 个样本的真值和预测位置向量，$N$ 表示样本总数。
	
	\item $err_{\text{T}}$: 表示真值与预测位置之间的绝对位置误差（单位：米）。该误差度量了预测位置与真实位置之间的欧几里得距离。
	\begin{equation}
		err_T = \frac{1}{N}\sum\limits_{i=1}^{N} \| r_{\text{true},i} - r_{\text{pred},i} \|_2
	\end{equation}
其中 $r_{\text{true},i}$ 和 $r_{\text{pred},i}$ 分别为第 $i$ 个样本的真值和预测位置向量，$N$ 表示样本总数。
	
	\item $err_{\text{ort}}^{\text{rad}}$: 通过比较预测姿态和真实姿态的相对方向来度量姿态误差，单位为弧度。该度量强调相对度量相比绝对度量的优势。定义如下：
	\begin{equation}
		err_{\text{ort, i}}^{\text{rad}} =  2 \cdot \arccos \left( \left| \langle q_{\text{pred},i}, q_{\text{true},i} \rangle \right| \right)
	\end{equation}
	
	\begin{equation}
		err_{\text{ort}}^{\text{rad}} = \frac{1}{N}\sum\limits_{i=1}^N err_{\text{ort, i}}^{\text{rad}}
	\end{equation}
	其中 $q_{\text{pred},i}$ 和 $q_{\text{true},i}$ 分别为第 $i$ 个样本的预测与真实姿态四元数，$N$ 表示样本总数，$\langle \cdot, \cdot \rangle$ 表示四元数的内积。
	
	\item $err_{\text{ort}}^{\circ}$: 以度数形式表示的 $err_{\text{ort}}$。定义如下：
	\begin{equation}
		err_{\text{ort, i}}^{\circ} =  \frac{180}{\pi}^{\circ} err_{\text{ort, i}}^{\text{rad}}
	\end{equation}
	\begin{equation}
		err_{\text{ort}}^{\circ} = \frac{1}{N}\sum\limits_{i=1}^N err_{\text{ort, i}}^{\circ}
	\end{equation}
	
	\item $score_{\text{pst}}$: SPEED 数据集中的位置评分，可以视为误差指标，数值越低表示误差越低。定义如下：
	\begin{equation}
		score_{\text{pst}} = err_{\text{t}}
	\end{equation}
	
	\item $score_{\text{ort}}$: SPEED 数据集中的姿态评分，其值越低表示误差越低。定义如下：
	\begin{equation}
		score_{\text{ort}} = err_{\text{ort}}^{\text{rad}}
	\end{equation}
	
	\item $score$: SPEED 数据集的总评分，将位置和姿态的结果进行综合，可以认为硕士6D姿态估计误差的综合指标。作为 6D 姿态估计的整体评价指标。定义如下：
	\begin{equation}
		score = score_{\text{ort}} + score_{\text{pst}}
	\end{equation}
	该综合评分在同时考虑位置和姿态准确度的情况下提供单一指标来评估整体表现，得分越低说明误差越低。
	
	\item $score_{\text{pst}}^+$: SPEED+ 数据集中的位置评分。定义如下：
	\begin{equation}
		score_{\text{pst, i}}^+ = 
		\begin{cases}
			0, & \text{if } err_{\text{t, i}} < 0.002173 \\
			err_{\text{t, i}}, & \text{otherwise}
		\end{cases}
	\end{equation}
	
	\begin{equation}
		score_{\text{pst}}^+ = \frac{1}{N}\sum\limits_{i=1}^N score_{\text{pst, i}}^+
	\end{equation}
	
	\item $score_{\text{ort}}^+$: SPEED+ 数据集中的姿态评分。定义如下：
	\begin{equation}
		score_{\text{ort, i}}^+ = 
		\begin{cases}
			0, & \text{if } err_{\text{ort, i}}^{\circ} < 0.169^\circ \\
			err_{\text{ort, i}}^{\text{rad}}, & \text{otherwise}
		\end{cases}
	\end{equation}
	\begin{equation}
		score_{\text{ort}}^+ = \frac{1}{N}\sum\limits_{i=1}^N score_{\text{ort, i}}^+
	\end{equation}
	
	\item $score^+$: SPEED+ 数据集官方评价中的综合评分，综合了基于位置和姿态的评分，可以认为是6D姿态估计的误差的综合指标，分数越低表示误差越低。定义如下：
	\begin{equation}
		score^+ = \frac{1}{N}\sum\limits_{i=1}^N(score_{\text{ort, i}}^+ + score_{\text{pst, i}}^+)
	\end{equation}
	\item 关键点i的像素误差 $\text{err}_{\text{kp},i}$: 表示所有样本中关键点 $i$ 在图像空间中的检测精度。定义如下： 
	\begin{equation} 
		\text{err}_{\text{kp},i,j} = \| p_{\text{pred},i,j} - p_{\text{true},i,j} \|_2 
	\end{equation}
	\begin{equation} 
		\text{err}_{\text{kp},i} = \frac{1}{N} \sum\limits_{j=1}^{N} \text{err}_{\text{kp},i,j}
	\end{equation}
	其中 $p_{\text{true},i,j}$ 和 $p_{\text{pred},i,j}$ 分别为第 $j$ 个样本中关键点 $i$ 的真值和预测像素坐标，$N$ 表示样本总数。
	
	\item 关键点总体平均像素误差 $\text{err}_{\text{kp}}$: 提供了关键点检测整体精度的综合度量。定义如下： 
	\begin{equation} 
		\text{err}_{\text{kp}} = \frac{1}{K} \sum\limits_{i=1}^{K} \text{err}_{\text{kp},i}
	\end{equation}
	其中 $K$ 表示关键点总数。
\end{itemize}

\subsection{关键点检测结果}
模型训练完成后，得到了如下的关键点检测效果。如图~\ref{fig:SPEED+_kpts_demo}所示，对于 SPEED+ 数据集，其中绿色的点表示关键点的真实位置，红色的点表示关键点的预测位置。可以看出关键点的预测位置基本上与关键点的真实位置重合，即使是在光照不足、目标截断以及地球背景干扰的情况下。如图~\ref{fig:Nauka_MLM_kpts_demo}和图~\ref{fig:Starlink_kpts_demo}所示，Nauka MLM 与 Starlink 的关键点预测位置也大体上与其真实值重合。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/SPEED+kptsdemo.png}
	\caption{SPEED+ 数据集关键点检测效果可视化}
	\label{fig:SPEED+_kpts_demo}
	\vspace{0pt}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/NaukaMLM_kptsdemo.png}
	\caption{Nauka MLM 数据集关键点检测效果可视化}
	\label{fig:Nauka_MLM_kpts_demo}
	\vspace{0pt}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/Starlink_kptsdemo.png}
	\caption{Starlink 数据集关键点检测效果可视化}
	\label{fig:Starlink_kpts_demo}
	\vspace{0pt}
\end{figure}

为了评估关键点检测的精度，这里统计了网络检测出的这三个数据集的每个关键点的像素误差 $\text{err}_{\text{kp},i}$。如图~\ref{fig:SPEED+_kpts_err}所示，关键点的检测误差大部分点均位于10个像素以下，有3个点的关键点检测精度在10到20像素之间，误差比较明显。根据模型的定义，这3个点是 SPEED+ 数据集 Tango 卫星的天线角点。由于天线比较细，在远处时特征变得模糊甚至几乎不可见，因此才导致了较大的误差。但是这些天线的特征在近处却可以有效利用。对于1900 \times 1200尺寸的图像而言，10到20像素之间的像素误差也是相对较小的。总体平均像素误差 $\text{err}_{\text{kp}}$只有8.70像素。

如图~\ref{fig:Nauka_MLM_kpts_err}所示，统计了Nauka MLM的每个关键点的像素误差$\text{err}_{\text{kp},i}$，绝大部分点依旧在10像素以下，对于像素误差大的关键点，其误差在20像素左右。相对于SPEED+数据的Tango而言，Nauka MLM这个航天器实际上是空间舱段，尺寸远比Tango大，因此数据集6D姿态中的位置设置的范围比SPEED+的大。而Nauka MLM也存在与Tango同样细的天线，这导致其在更远处检测关键点的难度进一步增加。然而，Nauka MLM却存在好几个关键点的检测精度超越了SPEED+的Tango，这与SPEED+表面特征的重复性较高、其方体结构接近于极性、关键点的区分度不是那么高有关。而Nauka MLM则由于其表面更复杂的特征，使得关键点之间的区分度更大。在 $\text{err}_{\text{kp}}$指标上，其误差只有7.82像素，小于SPEED+的 $\text{err}_{\text{kp}}$。

如图~\ref{fig:Starlink_kpts_err}所示，Starlink的$\text{err}_{\text{kp},i}$大多位于5以下，然而却出现了三个关键点的$\text{err}_{\text{kp},i}$明显偏大，达到了30像素左右。这是由于Starlink的太阳能板基座的反光率不高，导致远处时特征极为弱化。而其余关键点的$\text{err}_{\text{kp},i}$较低是因为其位于太阳能板的角点及其折叠线上，特征极为明显。其$\text{err}_{\text{kp}}$为10.83像素。总的来说，本章的关键点检测模型在不同数据集上的$\text{err}_{\text{kp}}$表现还算相对稳定，总体平均像素误差位于7.82到10.83个像素之间。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/SPEED+kpterrbar.png}
	\caption{SPEED+数据集关键点像素误差}
	\label{fig:SPEED+_kpts_err}
	\vspace{-20pt}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/Nauka MLM_kpt_err_bar.png}
	\caption{Nauka MLM数据集关键点像素误差}
	\label{fig:Nauka_MLM_kpts_err}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/Starlinkkpt_err_bar.png}
	\caption{Starlink数据集关键点像素误差}
	\label{fig:Starlink_kpts_err}
\end{figure}



\subsection{EfficientViT 消融实验}
这里进行了 EfficientViT 的消融实验，使用三个数据集和三个度量指标 $score_{\text{pst}}^+$、$score_{\text{ort}}^+$ 以及 $score^+$（分别表示平移、旋转和总体误差），对比了 EfficientViT 与 YOLOv8n-Pose 原始骨干网络的性能。后面三个消融实验的表中的 YOLOv8 都指的是 YOLOv8 没有改动时的原始结构。而EfficientViT， Triplet Attention 均是对YOLOv8相应模块进行改造后的结果。实验数据如表~\ref{tab:EfficientViTAblation} 所示。实验结果表明，虽然 EfficientViT 的参数量增加了约 30\%，但在大多数数据集上，其误差均得到了显著降低。

对于 Nauka MLM 数据集，误差指标$score_{\text{pst}}^+$ 下降了 18.60\%，$score_{\text{ort}}^+$ 下降了 16.48\%，而 $score^+$ 则下降了 16.92\%。对于 Starlink 数据集，误差下降效果更加明显，$score_{\text{pst}}^+$、$score_{\text{ort}}^+$ 和 $score^+$ 分别下降了 36.51\%、24.60\% 和 28.78\%。SPEED+ 数据集的误差下降最为明显，$score_{\text{pst}}^+$、$score_{\text{ort}}^+$ 和 $score^+$ 分别下降了 39.63\%、39.68\% 和 39.67\%。

尽管大多数样本表现出较高的估计精度，但选择了一些误差较为明显的案例进行可视化，以更好地展示消融实验的影响。可视化图像中，白色实线框与红绿蓝实线箭头为 6D 姿态的真值，白色虚线框与红绿蓝虚线箭头为 6D 姿态的预测值。如图~\ref{fig:backbone_abliation} 所示，在所有三个数据集中，采用 EfficientViT 后预测得到的红色姿态框和姿态箭头与真实值更加吻合。在一些样本中，例如 SPEED+ 数据集中的第四个案例，YOLOv8 骨干网络在 6D 姿态估计中表现出明显的向左旋转偏差；而采用  EfficientViT 的骨干网络有效缓解了这一问题，其姿态估计结果与真实值高度一致。

对于原本存在大量样本显示出明显平移和旋转偏差的 Starlink 数据集，在引入 EfficientViT 后得到了显著改善。这些结果突显了 EfficientViT 中级联群注意力机制在提升姿态估计精度方面的有效性。特别是 SPEED+ 数据集，展现了最为显著的提升。这主要归因于该数据集所面临的独特挑战，不仅包括多样化的目标姿态变化，还存在地球背景干扰。传统的 YOLOv8 骨干网络可能难以在这些复杂条件下集中于目标区域的关键点提取。相比之下，引入的 EfficientViT 利用其自注意力机制，即便在存在背景干扰的情况下，也展现出更强的聚焦于目标区域关键点提取的能力。这凸显了 EfficientViT 在处理复杂场景时的鲁棒性，以及其在显著提升非合作空间物体 6D 姿态估计精度方面的潜力。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/backbone_abliation.pdf}
	\caption{骨干网络消融实验效果可视化}
	\label{fig:backbone_abliation}
	\vspace{-49pt}
\end{figure}


\begin{table}[!htbp]
	\centering
	\caption{在SPEED+、Nauka MLM、Starlink 合成数据集上的 EfficientViT 消融实验}
	\label{tab:EfficientViTAblation}
	\setlength{\tabcolsep}{4mm}{
		% 原来是 {cccccc}，现在多了一列，因此改为 {ccccccc}
		\zihao{5} % 将表内字号设为五号
		\begin{tabular}{ccccccc}
			\toprule[1.5pt]
			数据集 & 骨干网络 & 参数量(M) & $Score^+_{pst}$ & $Score^+_{ort}$ & $Score^+$ \\
			\midrule[1pt]
			\multirow{2}{*}{SPEED+} 
			& EfficientViT & 4.2544 & \textbf{0.0099} & \textbf{0.0228} & \textbf{0.0327} \\
			& YOLOv8     & 3.2539 & 0.0164 & 0.0378 & 0.0542 \\
			\midrule[1pt]
			\multirow{2}{*}{Nauka MLM} 
			& EfficientViT & 4.2959 & \textbf{0.0105} & \textbf{0.0228} & \textbf{0.0334} \\
			& YOLOv8     & 3.2955 & 0.0129 & 0.0273 & 0.0402 \\
			\midrule[1pt]
			\multirow{2}{*}{Starlink} 
			& EfficientViT & 4.1960 & \textbf{0.0153} & \textbf{0.0337} & \textbf{0.0490} \\
			& YOLOv8     & 3.1956 & 0.0241 & 0.0447 & 0.0688 \\
			\bottomrule[1.5pt]
		\end{tabular}
	}% End of resizebox
\end{table}



\subsection{TANeck 消融实验}

这里对 TANeck 进行了消融实验，其方法与 EfficientViT 的消融实验类似，并使用了相同的三个数据集。实验结果如表~\ref{tab:TANeck_abliation}所示。应用 TANeck 后，各项指标均显著提升。对于 SPEED+ 数据集，误差指标$score_{\text{pst}}^+$、$score_{\text{ort}}^+$以及$score^+$分别下降了17.07\%、10.85\%和12.55\%。对于 Nauka MLM 数据集，这些指标分别下降了15.50\%、12.45\%和13.43\%，而在 Starlink 数据集中，这些指标的下降更为明显，分别达到了36.93\%、27.74\%和30.95\%。

Starlink 相对于 SPEED+/SPEED 中的 Tango 卫星以及 Nauka MLM 航天器，其表面的纹理等细粒度特征更难识别，太阳帆板的纹理模式显得较为重复，而标注的关键点又大多位于其表面。对于位于其本体部分的区域，由于材质的原因，使其反射率较低，通常呈现黑色，这也进一步加剧了纹理等细粒度特征识别的难度。传统的 YOLOv8 在这类目标的关键点检测上存在困难。而引入 Triplet Attention 构造的 TANeck 在细粒度特征提取能力上得到了明显的提升，因此才使得其在检测这类特殊目标时，提升幅度明显增大。

\begin{table}[!htbp]
	\centering
	\caption{TANeck消融实验}
	\label{tab:TANeck_abliation}
	{%
		\zihao{5} % 将表格内文字字号设为五号
		\setlength{\tabcolsep}{6.5mm}{
			\begin{tabular}{ccccc}
				\toprule[1.5pt]
				数据集 & Neck & $score_{\text{pst}}^+$ & $score_{\text{ort}}^+$ & $score^+$ \\
				\midrule[1pt]
				\multirow{2}{*}{SPEED+}
				& TANeck  & \textbf{0.0136} & \textbf{0.0337} & \textbf{0.0474} \\
				& YOLOv8  & 0.0164          & 0.0378          & 0.0542          \\
				\midrule[1pt]
				\multirow{2}{*}{Nauka MLM}
				& TANeck  & \textbf{0.0109} & \textbf{0.0239} & \textbf{0.0348} \\
				& YOLOv8  & 0.0129          & 0.0273          & 0.0402          \\
				\midrule[1pt]
				\multirow{2}{*}{Starlink}
				& TANeck  & \textbf{0.0152} & \textbf{0.0323} & \textbf{0.0475} \\
				& YOLOv8  & 0.0241          & 0.0447          & 0.0688          \\
				\bottomrule[1.5pt]
			\end{tabular}
		}
	}
\end{table}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/TANeck_Abliation.pdf}
	\caption{TANeck 消融实验结果可视化}
	\label{fig:TANeck_abliation}
	
\end{figure}


\subsection{A3DKS 对比实验} \label{A3DKS_Ablation_subsection}

本文将 A3DKS 损失函数与 YOLOv8-pose 中使用的 OKS 损失函数进行比较，以验证 A3DKS 的有效性。结果如表~\ref{tab:OKSvsA3DKS}所示：使用 A3DKS 后，所有数据集上的误差指标$score^+$与$score_{\text{ort}}^+$均有所下降。Nauka MLM 和 Starlink 上的下降更为显著，分别达到了32.97\%和21.70\%，而 SPEED+ 的下降仅为10.58\%。这主要是因为 Starlink 和 Nauka MLM 的目标形状较为细长，即使在目标位置保持不变、仅姿态发生变化时，其二维透视投影也会出现更明显的变化，而这正是 A3DKS 在平衡关键点 3D 尺度误差敏感性方面所能发挥优势的场景。


\begin{table}[htbp] 
	\centering
	\caption{损失函数消融实验}
	\label{tab:OKSvsA3DKS}
	{%
		\zihao{5} % 将表格内文字字号设为五号
		\setlength{\tabcolsep}{6mm}{
			\begin{tabular}{ccccc}
				\toprule[1.5pt]
				数据集 & 损失函数 & $score_{\text{ort}}^+$ & $score_{\text{pst}}^+$ & $score^+$ \\
				\midrule[1pt]
				\multirow{2}{*}{SPEED+}
				& OKS   & 0.0382          & \textbf{0.0164} & 0.0546 \\
				& A3DKS & \textbf{0.0339} & 0.0184          & \textbf{0.0523} \\
				\midrule[1pt]
				\multirow{2}{*}{Nauka MLM}
				& OKS   & 0.0272          & \textbf{0.0130} & 0.0402 \\
				& A3DKS & \textbf{0.0183} & 0.0137          & \textbf{0.0320} \\
				\midrule[1pt]
				\multirow{2}{*}{Starlink}
				& OKS   & 0.0446          & \textbf{0.0240} & 0.0686 \\
				& A3DKS & \textbf{0.0349} & 0.0243          & \textbf{0.0591} \\
				\bottomrule[1.5pt]
			\end{tabular}
		}
	}
\end{table}

如图~\ref{fig:A3DKS}所示，A3DKS 损失在三种不同目标上的作用均使其 6D 姿态估计相较于 OKS 有一定程度的误差降低。通过分析旋转误差指标$score_{\text{ort}}^+$和位置误差指标$score_{\text{pst}}^+$，可以看出旋转估计误差显著下降，但位置误差指标略有下降。SPEED+ 的$score_{\text{pst}}^+$下降了9.76\%，而 Nauka MLM 和 Starlink 分别下降了6.20\%和1.24\%。这通常与关键点的误差敏感性以及旋转带来的"杠杆效应"相关：纵深分布越深的关键点在 3D 空间中离"支点"越远，因而图像平面上的微小像素偏差更容易引发另一端的明显变化。虽然 A3DKS 能在一定程度上抑制这类误差，但也有可能导致关键点在某个方向上的集中偏移，从而增加平移误差。尽管位置误差指标略有增加，综合得分$score^+$仍然有所提升，表明整体而言 A3DKS 具备更优的综合性能。
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/A3DKS_abliation.pdf}
	\caption{A3DKS对比实验结果可视化}
	\label{fig:A3DKS}
	\vspace{-20pt}
\end{figure}


\section{本章小结}
本章基于深度学习方法开展了空间非合作目标的关键点检测研究，针对传统网络 检测在光照剧变、目标截断等不利场景下精度不足的问题，选用 YOLOv8-Pose 作为基础框架，并在骨干网络和 Neck 结构中分别引入 EfficientViT 与 Triplet Attention 机制，从全局与局部两个层面提升网络的特征提取能力；同时改进关键点损失函数，提出A3DKS以更好地结合三维深度信息。通过 SPEED+ 与自建的 Starlink、Nauka MLM 数据集进行训练与验证，结果表明 EfficientViT 与 TANeck 在全局或局部特征关注能力的优势，二者在不同场景下均有效降低 6D 姿态估计误差；而 A3DKS 对通过引入针孔相机模型 3D 尺度缩放，在细长、存在关键点深度大尺度变化的目标上提升尤为明显，以上的改进为后续 6D 位姿解算提供了高精度的关键点检测结果。
