\chapter{基于注意力机制的空间非合作目标关键点检测}
\label{chap:attention_kpt}
\section{引言}
为了获取空间非合作目标的6D姿态，根据本研究的方案，首先需要从单目图像中检测出关键点，进而为后续的6D姿态解算提供必要的数据。
根据现有研究，关键点检测算法主要分为基于传统图像特征提取的检测方法（如尺度不变特征变换（Scale-Invariant Feature Transform, SIFT）\citep{sift}、快速特征检测（Features from Accelerated Segment Test, FAST）\citep{fast}和定向FAST和旋转BRIEF（Oriented FAST and Rotated BRIEF, ORB）\citep{orb}）与基于神经网络的检测方法。前者通过一些较为固定的特征进行检测，从算法层面而言其计算效率相对较高。然而，由于空间非合作目标本身可供利用的特征较为复杂，特征匹配较为困难，且受到目标自身姿态变化的影响，其特征难以保持稳定。再加上目标截断或剧烈光照变化（如过曝或欠曝）等情况，使得特征变得不可识别，导致实际应用精度相对有限。
而基于神经网络的方法，通过合理选取和设计网络结构，可以使其对关键点的复杂特征具有很强的拟合能力，同时对于空间环境中的光照变化表现出较强的鲁棒性，因而精度相对较高。效率问题则可以通过轻量化的网络设计得到解决。本章将从深度学习方法出发进行关键点检测模型的设计，同时考虑到空间非合作目标所处环境和本身的特殊性，引入注意力机制来改进神经网络，从而增强其鲁棒性，有效提升检测精度。
\section{基于深度学习的关键点检测概述}
在基于深度学习的关键点检测领域，尤其是人体姿态估计领域，涌现了许多经典网络，这些经典网络已被迁移到各种领域的关键点检测任务中，包括空间非合作目标的6D位姿估计。2014年，Toshev等人首次将卷积神经网络（Convolutional Neural Network, CNN）引入人体关键点检测，构建了DeepPose网络，这是一个直接回归关键点坐标的多层CNN级联网络\citep{toshev2014deeppose}。然而，直接从图像回归关键点坐标而不进行进一步处理可能无法达到足够高的精度。
另一种方法是修改经典的计算机视觉网络来回归关键点坐标。例如，YOLO（You Only Look Once）目标检测网络预测点坐标的能力可以转移到预测关键点坐标。2022年，Maji等人提出了YOLO Pose \citep{yolo_pose}，它在YOLO目标检测框架中添加了关键点预测模块。YOLO Pose引入了一种新的损失函数，称为目标关键点相似度损失（Object Keypoint Similarity Loss, OKS Loss），使模型能够在训练期间直接优化关键点的精度。同时YOLO Pose注重实时性能。与传统方法依赖测试时数据增强和多尺度测试不同，YOLO Pose不依赖任何测试时增强，从而实现更快的推理速度，非常适合实时应用。对于空间非合作目标，其实时能力使航天器能够实时响应非合作目标的6D位姿。
非合作空间目标是不会变形的刚体，因此从刚体研究中获得启发。2018年，Tekin等人提出了一个基于YOLO架构的6D位姿估计网络，该网络直接回归包围物体的边界框的八个顶点和中心的坐标，以估计物体的6D位姿\citep{tekin2018real}。由于每个目标只需预测九个点，该方法表现出良好的实时性能，但九个点的数量略显不足，在遇到目标截断问题时容易出现PnP（Perspective-n-Point）求解困难。
然而，从2D图像到关键点坐标的映射是一个高度非线性的映射，神经网络可能难以学习这种映射，导致低精度预测。为解决这个问题，提出了另一类基于关键点热图预测的解决方案。同样在2014年，Tompson, Jain等人引入了基于热图的关键点预测，生成一个热图，其中每个像素代表该像素作为关键点的概率\citep{tompson2014joint}。通过求解热图中的局部最大值点，可以获得关键点坐标。由于该方法充分利用了每个像素的信息，因此可以做出更准确的预测。
2016年，密歇根大学的研究人员提出了沙漏网络结构（Hourglass Network）\citep{newell2016stacked}，该结构堆叠了几个大小先增后减的沙漏网络，使网络能够捕获多尺度信息。此外，堆叠的多层结构在每一层都持续重新定位。2020年，Wang等人认为高分辨率表示对位置敏感的视觉任务至关重要，提出了高分辨率网络（High-Resolution Network, HRNet）\citep{wang2020deep}，该网络在整个过程中保持高分辨率表示。它并行连接从高分辨率到低分辨率的卷积流，并在不同分辨率之间反复交换信息。其优势在于获得更丰富的语义和更准确的空间表示。
然而，为了获取相对较高的检测精度并非只能采用热图的方式。一些研究表明\citep{mao2022poseur, 10394434, lu2024ksl}，在引入Transformer或采用一些先进的架构如YOLO后，直接回归关键点的网络仍能够取得不错的精度。


鉴于基于深度学习的关键点检测的广泛研究，其也自然而然拓展到了航天领域。Huan等人（2020）提出了一种基于单目相机图像的非合作航天器相对位置和姿态估计算法\citep{huan2020pose}。他们的方法结合了现成的目标检测网络、关键点回归网络和多视图三角测量来重建3D模型。该方法有效地集成了深度学习与几何优化算法，展示了深度学习在航天领域的创新应用。
Park等人（2019）引入了一种新颖的CNN架构和训练程序，用于非合作航天器的鲁棒准确位姿估计\citep{park2019towards}。他们的方法在斯坦福大学空间交会实验室和欧洲航天局主办的位姿估计挑战赛中取得了显著成功。通过利用首先检测物体然后回归2D关键点位置的CNN架构，他们在单图像位姿估计问题中展示了改进的性能。此外，他们引入了纹理随机化技术来增强CNN在空间图像上的训练，进一步提高了性能。
Wang等人（2022）使用Transformer模型重新审视了单目卫星位姿估计，旨在解决现有基于CNN方法的局限性，如对纹理的偏见和对绝对距离的间接描述\citep{wang2022revisiting}。通过设计基于关键点的有效卫星表示模型并构建端到端的关键点集预测网络，他们在基准数据集上取得了优越的性能，证明了他们方法的效率和有效性。
Lotti等人（2022）研究了使用视觉Transformer来弥合卫星位姿估计中的域差距。他们提出了利用Swin Transformers和对抗域适应的算法，在欧洲航天局（European Space Agency, ESA）的卫星位姿估计竞赛中取得了具有竞争力的结果\citep{lotti2022investigating}。他们的研究突出了视觉Transformer在解决卫星位姿估计任务中域偏移相关挑战方面的潜力。以上的研究为了达到较高的检测精度有相当一部分使用基于热图这种密集预测的方式，其计算代价相对较高。对于那些通过神经网络直接回归关键点的方式，其精度则不够理想。



\section{YOLOv8 网络简介}
为了避免热图这种密集预测的计算开销对实时性的影响，考虑到近年来提出的YOLOv8模型在实时性和精度上的出色表现，本章选用其作为关键点检测的基础模型。YOLOv8 是 Ultralytics 公司开发的新一代目标检测模型\cite{yolov8_ultralytics}，兼顾了检测精度和实时性能。通过改进检测头结构，它还可以用于关键点检测任务。模型整体由骨干网络（Backbone）、颈部网络（Neck）和解耦头（Head）三部分组成，其架构如图\ref{fig:YOLOv8_architecture} 所示。
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{Img/YOLOv8_architecture.png}
	\caption{YOLOv8架构}
	\label{fig:YOLOv8_architecture}
	\vspace{-3ex}
\end{figure}
Backbone 部分负责提取图像的基础特征，由多个关键模块堆叠构成，主要包括 Conv 模块、C2f、C2fres 模块和 SPPF（Spatial Pyramid Pooling-Fast）模块。Conv 模块是深度卷积神经网络中的基本单元，一般由卷积层、批归一化和非线性激活函数组成，用于提取局部图像特征并逐步缩小特征图尺寸。C2fres 模块是在C2f模块的基础上，在跨阶段部分结构的基础上引入残差连接的卷积块。该模块将输入特征通道分为两部分：一部分直接与输出相连形成快捷通道（残差链路），另一部分经过两层卷积及内部的小型残差单元处理后再与捷径通道特征在通道维度上拼接，最后通过1×1卷积融合输出。这样设计在减少冗余参数的同时保持了丰富的特征表达能力，是 YOLOv8 主干中的主要特征提取单元。SPPF 模块则采用改进的金字塔空间池化策略，通过多尺度的最大池化操作提取不同感受野的特征并将其拼接，从而增强对不同尺寸目标的感知能力。 

Neck部分借鉴了路径聚合网络（PANet）的思想，融合来自不同尺度的特征以强化表征能力。具体而言，Neck 包含上采样路径和下采样路径两部分：在上采样路径中，利用 C2f 模块和 Deconv 模块（反卷积模块）逐步将低分辨率的强语义特征上采样，并与 Backbone 不同阶段的对应特征图进行融合，丰富高分辨率特征的语义信息；而在下采样路径中，将上采样融合后的特征通过额外的 Conv 模块和 C2f 模块逐级下采样，并与 Backbone 中高层次的特征再次融合，形成多层级的综合特征。经过 Neck 的双向特征融合，模型获得了包含丰富语义和细节的多尺度特征供 Head 使用。 

Head 部分采用解耦式检测头结构，将分类和回归分支分离开来，分别输出目标的类别概率和边界框位置。在关键点检测任务中，Head 经过相应改造可以输出每个目标的关键点坐标。YOLOv8-Pose 即为 YOLOv8 的关键点检测版本，它通过在检测头中增加关键点坐标的预测分支，实现对目标关键点的精确定位。得益于高效的网络架构设计，YOLOv8 在精度与速度之间取得了良好平衡，因而在众多工业生产和日常生活场景中得到了广泛应用\cite{sohan2024review,s23167190,ani14121791,Dong2024,yuan2024yolov8}。

然而在空间非合作目标的应用场景中，比如光照的剧变，目标复杂的几何外形，不同位姿图像差异的明显等因素使得其应用还存在一定的挑战。为了解决这一挑战，本文引入了注意力相关的模块对Backbone和Neck进行了改造，使其更适应空间非合作目标的应用。


\section{基于注意力机制的关键点检测网络模型设计}
由于YOLOv8毕竟是通过直接回归关键点坐标的方式进行检测，其精度相较于热图预测这种方案来看，精度还是不够具有竞争力。考虑到注意力机制在计算机视觉领域的广泛应用，其通过改变网络对重要特征的关注度，使得其在改善网络精度上有着较为显著的效果。考虑到全局特征（航天器更为高层次的结构模式）以及局部特征（如航天器表面的纹理模式），都是关键点检测中重要的依据。在YOLOv8的架构中，Backbone是架构的核心部分xxxx。如图~\ref{fig:keypoint_detection_neural_network}中子图 (a) 所示，将EfficientViT引入到Backbone部分。{专家会说到为什么不把骨干网络的sppf替换掉呢，为什么把EfficientViT最后接一个sppf}， 考虑到细粒度特征的重要性，本章在颈部网络 
\subsection{EfficientViT网络}

如图~\ref{fig:keypoint_detection_neural_network} 中子图 (a) 所示，本文将EfficientViT引入YOLOv8-pose的骨干网络，以增强其对全局特征的关注度，适应空间非合作目标存在的目标截断、光照剧变等因素造成的局部特征无法利用从而更依赖于全局特征的情况。EfficientViT\cite{liu2023efficientvit}是一种针对视觉Transformer (Vision Transformer, ViT) 的高效架构，以显著降低计算和存储开销。其核心是在模型中引入新的基础模块 EfficientViT 块，如图~\ref{fig:keypoint_detection_neural_network} 中子图 (b) 所示，每个模块采用三明治式布局（Sandwich Layout）结构，结合级联组注意力（Cascaded Group Attention, CGA）机制，并辅以参数重分配策略，实现模型的轻量化设计。整个 EfficientViT 网络将上述高效模块堆叠成多阶段结构：首先通过重叠式块嵌入将输入图像划分为若干patch并嵌入初始令牌序列，然后分三个阶段逐步对特征图进行降采样，同时逐层提高通道维度和注意力头数。每个阶段由若干个EfficientViT模块组成（后续阶段的模块数多于前期，以补偿特征图尺寸的降低），在阶段之间插入EfficientViT降采样块以减少分辨率并防止信息损失。EfficientViT在架构上用批归一化（BatchNorm）取代了传统Transformer中的层归一化（LayerNorm），并使用ReLU激活函数代替GELU，以便更好地适配高效推理。通过以上设计，EfficientViT 模型在保持或提升准确率的同时，大幅减少了参数量和计算量，兼顾了精度与效率。

如图~\ref{fig:keypoint_detection_neural_network} 中子图 (b) 所示：Sandwich Layout结构采用了夹层式的Transformer结构，以提高内存访问效率并强化通道信息交流。具体来说，在该模块中仅保留一次多头自注意力（Multi-Head Self-Attention, MHSA）计算，并在其之前和之后分别堆叠多个前馈网络（Feed-Forward Network，FFN）层，相当于用FFN"夹住"少量注意力层。从数学公式上，可将第 $i$ 个模块的变换表示为：

\begin{equation}\label{eq:sandwich}
	X_{i+1} = \Phi^F_i\Big(\Phi^A_i\big(\Phi^F_i(X_i)\big)\Big)\,
\end{equation}

其中 $\Phi^A_i$ 表示第 $i$ 个模块中的自注意力变换，$\Phi^F_i$ 表示前馈网络变换（包含非线性激活和逐元素加法的两层感知机，膨胀比例适当缩小以减小参数冗余），$X_i$ 和 $X_{i+1}$ 分别是模块输入和输出特征。与传统Transformer块每层都执行一次注意力不同，Sandwich布局大幅减少了记忆开销高的全局注意力计算次数，将注意力"稀释"到更少的层中。这种设计显著降低了由于多头注意力读写大规模特征图所带来的内存访问延迟，并通过增加FFN层数来保持模型的表达能力和通道通信效率。在Sandwich结构的作用下，EfficientViT模块用更多的参数高效的FFN计算取代了大部分注意力计算，使模型在内存和计算开销上更为高效。

对于其级联组注意力（CGA）机制，在标准ViT的多头自注意力中，不同注意力头往往学到相似的注意力图，存在计算冗余。CGA针对此问题对注意力计算方式进行重构，大幅提升计算效率和多样性。其核心思想是将输入特征按通道维度均分为 $h$ 个子组，每个子特征组仅供对应的注意力头计算。这样，每个注意力头只关注输入的一部分通道信息，从而避免了以往每个头都对全通道特征重复计算的浪费。形式上，对于第 $i$ 个模块、划分后的第 $j$ 个特征子组 $X_{ij}$（$j=1,2,\dots,h$），CGA的计算过程可用数学公式描述为：

\begin{equation}\label{eq:cga1}
	X^{e}_{i,j} = \text{Attn}\!\Big(X_{ij}W^{Q}_{ij},\; X_{ij}W^{K}_{ij},\; X_{ij}W^{V}_{ij}\Big)
\end{equation}
\begin{equation}
	X^e_{i+1} = \text{Concat}\big[X^e_{i,1}, X^e_{i,2}, \dots, X^e_{i,h}\big] \, W^P_i\
\end{equation}

其中 $\text{Attn}(Q,K,V)$ 表示标准的自注意力计算，$W^Q_{ij},W^K_{ij},W^V_{ij}$ 为第 $i$ 个模块中第 $j$ 个头的查询、键、值投影矩阵，$W^P_i$ 为输出投影矩阵，将所有头的输出特征串联后映射回原始维度。上述分组方案类似于卷积中的组卷积思想：由于每个头只处理$\frac{1}{h}$的通道，其 Q/K/V 投影的参数量和矩阵乘法计算量也相应减少到原来的 $\frac{1}{h}$，从而理论上令注意力模块在参数和算力开销上缩减近 $h$ 倍。单纯分组虽减少了开销，但可能因每个头仅看局部通道而降低模型表示能力。为此，CGA 进一步引入级联机制来弥补信息交流：将前一注意力头的输出逐步传递并融合到下一头的输入中。用公式表示，即对于 $1<j\le h$，令

\begin{equation}\label{eq:cga2}
	X'_{i,j} = X_{ij} + X^e_{i,\,j-1}\,
\end{equation}

其中 $X'_{i,j}$ 表示更新后的第 $j$ 个子特征组，再将其用于计算第 $j$ 个头的注意力（式(\ref{eq:cga1})）。通过这种逐头累加的级联连接，每个后续注意力头在关注自身子特征的同时，额外融合了前一头输出的全局信息，逐步丰富注意力的表示能力。CGA 还在每个头的查询变换之后附加了一层深度卷积（与前述 Sandwich 模块中的 DWConv 类似）以结合局部相关性，从而使注意力计算能够同时捕获全局和局部依赖关系。得益于以上设计，CGA 带来多重性能提升，各头处理不同子空间的特征显著提高了注意力图的多样性，减少了头之间的信息冗余，由于减少了每个头的通道维度，注意力计算的 FLOPs 和参数量下降明显。

传统的ViT采用单阶段固定长度的序列表示，多个Transformer编码器层堆叠，每层包含一次MHSA和一次FFN；而 EfficientViT 采用多阶段金字塔结构，特征图分辨率逐级降低、通道逐级增加，各阶段使用高效的EfficientViT块替代标准Transformer层。每个EfficientViT块仅含一层注意力（相比ViT每层都有）却包含双倍数量或以上的FFN变换，并通过DWConv引入卷积特征，这使得每层的特征提取能力得到了保证。

\subsection{TANeck}
细粒度特征在关键点检测中具有重要作用，尤其针对如图~\ref{fig:keypoint_detection_neural_network}的子图 (a) 所示纹理特征不明显的空间非合作目标。这类目标的关键点检测难度较大。因此，为增强YOLOv8的Neck模块对该类目标的特征提取能力，本文将 Triplet Attention \cite{Misra_2021_WACV} 机制集成其中，构建了 TANeck。Triplet Attention 是一种 CNN 特征域的再加权注意力机制，对细粒度特征具有更强的关注能力。TANeck的基础框架采用了路径聚合网络（PANet）\cite{panet}，这不同于传统仅自上而下的FPN结构\cite{fpn}。PANet 通过自底向上和自顶向下的双向特征传递机制丰富了特征层次结构。该双向增强方法有效保留了高分辨率细节，对于识别空间目标的复杂特征至关重要。此外，PANet 架构在解决传统单向 FPN 中常见的语义差距问题的同时，确保深层高层次上下文信息能够更早且更高效地融合。在非合作空间目标的检测任务中，这一点尤为关键，因为关键点的精确定位直接决定了检测性能。

Triplet Attention 机制如图~\ref{fig:keypoint_detection_neural_network}的子图 (c) 所示，由三个并行分支组成，可对输入特征图 $\mathbf{X} \in \mathbb{R}^{C \times H \times W}$ 进行跨维度的注意力建模。其核心思想是在计算开销几乎不增加的情况下，实现高效的跨维度特征交互。

第一个分支用于建模通道与宽度维度之间的交互。具体而言，首先交换输入张量的维度，得到 $\mathbf{X}_{1} \in \mathbb{R}^{W \times C \times H}$。然后，对 $\mathbf{X}_{1}$ 执行 Z-Pool 操作，并通过卷积提取通道-宽度维度的交互信息，最后经由 Sigmoid 激活函数获得注意力图：
\begin{equation} 
	\mathbf{M}_{1} = \sigma\!\Big(f_{1}\big(\text{Z-Pool}(\mathbf{X}_{1})\big)\Big)\,
\end{equation}
其中，$\sigma$ 表示 Sigmoid 激活函数，$f_{1}$ 表示第一个分支中的卷积操作，$\text{Z-Pool}$ 表示结合最大池化和平均池化的操作。

类似地，第二个分支关注通道与高度维度之间的交互。通过重新排列输入张量得到 $\mathbf{X}_{2} \in \mathbb{R}^{H \times C \times W}$，并对其执行与第一个分支相同的操作以获得注意力图：
\begin{equation} 
	\mathbf{M}_{2} = \sigma\!\Big(f_{2}\big(\text{Z-Pool}(\mathbf{X}_{2})\big)\Big)
\end{equation}

第三个分支直接作用于原始输入张量 $\mathbf{X}$，采用类似 BAM 的常规注意力机制得到注意力图：
\begin{equation} 
	\mathbf{M}_{3} = \sigma\!\Big(f_{3}\big(\text{Z-Pool}(\mathbf{X})\big)\Big)
\end{equation}

上述三个分支采用相同的注意力计算机制，但分别专注于不同维度间的交互。$\text{Z-Pool}$ 操作通过在通道维度上执行最大池化和平均池化，对该维度进行压缩，其计算方式如下：
\begin{equation} 
	\text{Z-Pool}(\mathbf{X}) = \text{Concat}\Big[\text{MaxPool}_{\text{Channel}}(\mathbf{X}),\ \text{AvgPool}_{\text{Channel}}(\mathbf{X})\Big]\,, 
\end{equation}
其中，$\text{Concat}$ 表示在通道维度上的拼接。最大池化和平均池化操作分别定义如下：
\begin{equation} 
	\text{MaxPool}_{\text{Channel}}(\mathbf{X}) = \max_{1 \le c \le C} \mathbf{X}_{c,:,:}
\end{equation}
\begin{equation} 
	\text{AvgPool}_{\text{Channel}}(\mathbf{X}) = \frac{1}{C} \sum_{c=1}^{C} \mathbf{X}_{c,:,:}
\end{equation}

接下来，将上述三个分支得到的注意力图逐元素相乘，融合生成最终的注意力图：
\begin{equation} 
	\mathbf{M} = \mathbf{M}_{1} \otimes \mathbf{M}_{2} \otimes \mathbf{M}_{3}\,
\end{equation}
其中，$\otimes$ 表示逐元素相乘。然后利用该融合后的注意力图对原始特征进行重新加权：
\begin{equation} 
	\mathbf{Y} = \mathbf{X} \odot \mathbf{M}\,
\end{equation}
其中，$\odot$ 同样表示逐元素相乘。Triplet Attention 机制在未显著增加计算复杂度的前提下，实现了跨维度特征的高效交互，显著增强了Neck的特征提取能力，并已将该模块成功集成至YOLOv8-pose框架中。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/kpts_detection_overview.png}
	\caption{(a) 关键点检测网络架构；(b) EfficientViT 块架构；(c) Triplet Attention 架构}
	\label{fig:keypoint_detection_neural_network}
	\vspace{-3ex}
\end{figure}


\subsection{全局注意力与局部注意力的联合作用}
当 EfficientViT 通过其多头自注意力机制提供全局上下文信息时，TANeck 则在高层特征通路中利用Triplet Attention机制来增强关键的局部细节。具体而言，EfficientViT 所捕捉的全局依赖关系能够在目标航天器仅部分可见时减少关键点检测的歧义，从而保证网络即使在目标因超出视野被截断，也能对整体形态进行合理推断。而 TANeck 则通过在不同尺度上选择性地细化特征，保留那些在纯 Transformer 设计中可能被忽视的重要空间线索。

这种互补对于具有重复纹理的、大尺寸或细长航天器目标，或对于目标几何结构超出相机视场范围的情形尤为受益：一方面，EfficientViT 可确保对航天器整体形状与不同6D姿态2D图像的稳健表征；另一方面，TANeck 的三元注意力能进一步强化那些对关键点精确回归至关重要的细节。因此，二者协同配合能够在具有挑战性的非合作航天场景下，实现更均衡、更全面的特征表征，保证了关键点检测的精度。从而能够大幅提升6D姿态估计的精度。
\subsection{A3DKS 损失}


YOLO-Pose 将 YOLOv5 架构扩展到人体姿态估计领域，通过检测人体关键点实现 \citep{yolo_pose}。该方法引入了用于关键点拟合的对象关键点相似度（OKS）损失函数，其公式为：

以下是修改后的LaTeX代码，使用$d_i$简化OKS公式：


\begin{equation}
	\mathrm{OKS} = \frac{\sum\limits_{i=1}^{N} \exp\left(-\frac{d_i^2}{2\, s^2 \, k_i^2}\right) \cdot v_i}
	{\sum_{i=1}^{N} v_i}
\end{equation}
其中：
\begin{itemize}
	\item $N$ 表示关键点的总数；
	\item $x_i$, $y_i$ 为预测的第 $i$ 个关键点的坐标；
	\item $x_i^{gt}$, $y_i^{gt}$ 为第 $i$ 个关键点的真实（ground truth）坐标；
	\item $d_i^2 = (x_i - x_i^{gt})^2 + (y_i - y_i^{gt})^2$ 是预测与真实关键点之间的欧氏距离的平方；
	\item $k_i$ 是与第 $i$ 个关键点对应的标准差常数（sigma），反映了该关键点标注的不确定性；
	\item $s$ 是尺度因子，通常与目标的尺寸相关（例如可以定义为目标边框面积的平方根）；
	\item $v_i$ 表示关键点的可见性，当第 $v_i=0$ 时表示关键点不可见，即位于图像范围外。当$v_i=1$时，表示关键点可见，即位于图像范围内。 
\end{itemize}
```

现在OKS公式直接使用$d_i^2$表示距离，而在下方解释中保留了$x_i$和$y_i$的定义。
对应的 OKS 损失定义即为如下
\begin{equation}
	\mathcal{L}_{\text{OKS}} = 1 - \mathrm{OKS}
\end{equation}


针对空间目标关键点检测中的特殊问题，本文提出了 A3DKS（Approximate 3D Keypoint Similarity）损失函数。该损失函数结合了小孔相机模型如图\ref{fig:pin_hole_model}，将关键点的深度信息纳入损失计算中，更准确地反映三维空间中的误差。A3DKS 损失函数的公式为：

\begin{equation}
	L_{kpts} = 1 - \frac{\sum\limits_{i=1}^{N} \exp \left( -\left( \frac{d_i Z_i}{f} \right)^2 \cdot \frac{K}{2 s_h^2 k_i^2} \right) v_i}{\sum\limits_{i=1}^{N_{kpts}} v_i}
\end{equation}
其中：

\begin{itemize}
	\item $d_i = \sqrt{(x_i - x_i^{gt})^2 + (y_i - y_i^{gt})^2}$，为第 $i$ 个关键点像素坐标的误差。
	\item $Z_i$：第 $i$ 个关键点的深度值。
	\item $f$：相机的焦距。
	\item $K$：常数系数，用于调整损失的尺度。
	\item $s_h$：目标面积的调和平均。
	\item 其他符号同前。
\end{itemize}

该损失函数通过引入深度信息 $Z_n$，有效地将二维位移误差映射到三维空间，提高了损失对不同深度关键点的敏感度，增强了模型对空间目标的检测精度。

\section{数据集的预处理与制作}
为了验证本章算法的性能，这里选用了公开数据集 SPEED+ 与自建的数据集 Starlink 和 Nauka MLM。
\subsection{SPEED+数据集简介}
SPEED+数据集\cite{speed+}是欧空局在以PRISMA任务重的Tango卫星(图~\ref{fig:tango})为对象的6D姿态估计挑战赛中提出的数据集，它是在上一代SPEED\cite{SPEED-Dataset}数据集上对数据集的样本进行了扩充和丰富。其中包含了六万张左右的合Tango卫星的合成图像。该数据集为深度学习模型提供了可大规模获取的合成训练样本和相应的高精度姿态标注。SPEED+在SPEED基础上进一步扩充了合成图像数量和多样性，为算法训练与消融实验提供了更丰富、稳定的样本来源。其数据集中的Tango卫星的6D姿态图像如图~\ref{fig:tango_demo}所示，数据集重包含了各种有挑战性的情境。子图(a)为Tango卫星位姿图像常规的情形，可以看出Tango卫星或远或近，姿态各异。与地面目标不同，空间中的三自由度变换变得更为明显，目标容易以各种朝向对准观测者。子图(b)为Tango卫星位姿图像中目标因部分超出视野而被截断的情形，这导致了部分特征的不可见。子图(c)Tango卫星出现在地球这个大背景下的情形。相对于纯黑色的深空，这种情形的区分度不是那么明显，且Tango的距离过远也容易混入背景导致难以识别。图(d)为光照过暗目标不清晰的情形，这种情况下Tango的特征被弱化，使其识别会存在困难。这些情境尤其是(b)(c)(d)会对关键点检测任务造成挑战。也真是因为这个数据集存在着这些挑战，也才能充分验证本章模型对这些情况的鲁棒性，因此本章选用了这着两个数据集来训练和测试本章的关键点检测模型。通过以上的消融实验来证明相应模块或者损失函数改进的有效性。
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\textwidth]{Img/tango.jpg}
	\caption{PRISMA任务中的Tango}
	\label{fig:tango}
\end{figure}


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{Img/tango_demo.png}
	\caption{(a)常规情形；(b)目标截断情形；(c)背景干扰情形: (d)光照过暗情形}
	\label{fig:tango_demo}
\end{figure}
\subsection{SPEED+数据集关键点标注制作}
由于SPEED+数据集缺乏空间非合作目标的关键点标注，因此这里将根据一定的特征手动选择关键点，并采用多视图几何的方法测算出关键点的3D坐标，从而能够进一步通过数据集的6D姿态标注和相机内参计算出关键点的像素坐标。

首先进行关键点3D坐标的选取与计算操作。为了使得关键点在图像中更容易被检测出，本章选取了Tango卫星的明显角点（如天线顶点，主体框架的角点）为关键点。接下来针对选取的关键点进行逐一的关键点3D坐标的计算。
设在空间非合作目标上选定的某个关键点，其真实3D坐标（在目标自身坐标系或某一公共坐标系下）为
\begin{equation}
	P_a = (X_a,\; Y_a,\; Z_a,\; 1)
\end{equation}
在某张2D图像（索引记为 $ab$）中，对应的像素坐标测量值记作
\begin{equation}
	\omega_{ab} = \bigl(x_{ab},\; y_{ab}\bigr)
\end{equation}
根据数据集的6D姿态标注可知：目标与相机之间的相对姿态可由旋转 $R_{ab}$ 和平移 $t_{ab}$ 来描述；结合相机内参矩阵 $K$，便可构造投影矩阵
\begin{equation}\label{eq:proj_ab_def}
	\mathrm{Proj}_{ab} \;=\; K\,\bigl[\,R_{ab}\;\big\vert\;t_{ab}\bigr]
\end{equation}
其中 $R_{ab}\in\mathbb{R}^{3\times 3}$，$t_{ab}\in\mathbb{R}^{3\times 1}$，$K$ 是已知的相机内参矩阵（3$\times$3）。

3D点 $P_a$ 与其在2D图像上的投影 $P_{ab} = (x_{ab},\,y_{ab},\,1)$ 应有关系：
\begin{equation}
	P_{ab} 
	\;\sim\; 
	\mathrm{Proj}_{ab}\,P_a
	\;=\;
	K\,[\,R_{ab}\mid t_{ab}\,] 
	\begin{bmatrix}
		X_a\\[4pt]
		Y_a\\[4pt]
		Z_a\\[4pt]
		1
	\end{bmatrix}
\end{equation}
其中"$\sim$"表示二者仅相差一个非零尺度因子（因为图像坐标用齐次坐标表示时，$(x,\,y,\,1)$ 与 $\lambda(x,\,y,\,1)$ 表示同一个像素点）。为将其转化为可线性求解的形式，通常对下述两者做叉乘或分列，得到线性方程。将
\begin{equation}
	\bigl(x_{ab},\;y_{ab},\;1\bigr)^\mathrm{T}
	\quad\text{与}\quad 
	\mathrm{Proj}_{ab}\,P_a
\end{equation}
做叉乘可得：
\begin{equation}
	\bigl(x_{ab}\,\mathrm{Proj}_{ab3} \;-\; \mathrm{Proj}_{ab1}\bigr)\,\bigl[X_a,\;Y_a,\;Z_a,\;1\bigr]^\mathrm{T} \;=\; 0
\end{equation}
\begin{equation}
	\bigl(y_{ab}\,\mathrm{Proj}_{ab3} \;-\; \mathrm{Proj}_{ab2}\bigr)\,\bigl[X_a,\;Y_a,\;Z_a,\;1\bigr]^\mathrm{T} \;=\; 0
\end{equation}
其中，$\mathrm{Proj}_{ab1}$、$\mathrm{Proj}_{ab2}$、$\mathrm{Proj}_{ab3}$ 分别表示投影矩阵 $\mathrm{Proj}_{ab}$ 的第1、2、3列。

若某关键点在 $g$ 幅图像中均可观测，则可从每幅图像中各获得两条线性约束，合计 $2g$ 条，将它们堆叠后记作
\begin{equation}\label{eq:Aeq0}
	A 
	\begin{bmatrix}
		X_a\\[3pt]
		Y_a\\[3pt]
		Z_a\\[3pt]
		1
	\end{bmatrix}
	=
	0
\end{equation}
其中，$A \in \mathbb{R}^{2g\times 4}$，每两行对应一幅图像的投影方程。为确定 \((X_a,\,Y_a,\,Z_a,\,1)\)，需要至少 \(g \ge 2\) 幅图像才能获得非平凡解。

由于向量 \(\bigl(X_a,\,Y_a,\,Z_a,\,1\bigr)^{\mathrm{T}}\) 处于 \(A\) 的零空间，而 \(A\) 的秩通常为 3（或接近 3），其零空间是一维空间。利用奇异值分解（SVD）可直接得到该零空间的基向量：
\begin{equation}
	A = U\,\Sigma\,V^\mathrm{T}
\end{equation}
若奇异值按从大到小排列，则 \(V\) 的最后一列生成 \(A\) 的零空间，记作 \(\mathbf{v}_4\)。因此
\begin{equation}
	\bigl(X_a,\,Y_a,\,Z_a,\,1\bigr)^\mathrm{T}
	\;\sim\;
	\mathbf{v}_4
\end{equation}

确定关键点的三维坐标 \(P_a\) 后，可基于各图像的投影矩阵 \(\mathrm{Proj}_{ab}\)
（其显式形式为 \(K\,[\,R_{ab}\mid t_{ab}\,]\)）将该点投影到相应图像平面：
\begin{equation}
	\bigl(x_{ab},\,y_{ab},\,1\bigr)^\mathrm{T}
	\;\sim\;
	\mathrm{Proj}_{ab}\,P_a
\end{equation}
在此基础上，便可在每张图像上自动生成关键点的 2D 像素标注，用于后续的监督或标注文件制作，大幅降低人工标注的工作量并提升了标注精度。





\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{Img/SPEEDkpt3d.png}
	\caption{SPEED+数据集Tango卫星的关键点}
	\label{fig:SPEEDpluskpt}
\end{figure}

\subsection{自建数据集的仿真渲染}

为了验证算法的泛化能力，本章另外选取了Nauka MLM与Starlink两种不同几何形状的航天器模型来制作空间非合作目标6D姿态数据集。

为了渲染出较高质量的数据集，这里选用了Blenderxxxx
为了制作出Nauka MLM与Starlink的6D姿态数据集，本章选用了BlenderProc对这两款模型进行渲染从而得到其6D姿态数据集。BlenderProc是一款基于Blender的开源渲染工具，用于生成物理逼真的合成图像，广泛应用于计算机视觉和机器学习领域的数据集生成。它提供了Python端API来调用Blender的基本功能，从而易于自动化生成大量仿真数据集。本文将其用于生成大规模的空间非合作目标6D姿态数据集。

\subsection{渲染参数设置}
在数据集生成过程中，本文使用 BlenderProc 对渲染参数进行设置，包括空间环境参数和相机参数。相机内外参如表 \ref{tab:camera-param} 所示，空间环境与光照参数如表 \ref{tab:env-param} 所示。下面分别给出各参数表，并对其含义进行简要解释。
\begin{table}[hbt]\centering
	\caption{相机参数}
	\label{tab:camera-param}
	\begin{tabular}{lcc}
		\toprule
		参数 & 数值 & 单位 \\
		\midrule
		图像分辨率 
		& $1920 \times 1200$ 
		& 像素 \\
		焦距($f_x, f_y$) 
		& $(3003.413,\;3003.413)$
		& 像素 \\
		主点($c_x, c_y$) 
		& $(960,\;600)$ 
		& 像素 \\
		径向畸变系数($k_1, k_2, k_3$) 
		& $(-0.249855,\;0.102193,\;-0.0210435)$ 
		& — \\
		切向畸变系数($p_1, p_2$) 
		& $(0,\;0)$ 
		& — \\
		相机位置 (X, Y, Z) 
		& $(0,\;0,\;0)$ 
		& 米 \\
		相机朝向 (欧拉角XYZ) 
		& $(0^\circ,\; -180^\circ,\; 0^\circ)$ 
		& 度 \\
		\bottomrule
	\end{tabular}
\end{table}
%-------------------------------------------------------------
% 空间环境与光照参数表
\begin{table}[hbt]\centering
	\caption{空间环境与光照参数}
	\label{tab:env-param}
	\begin{tabular}{lc}
		\toprule
		参数 & 数值 \\
		\midrule
		背景颜色(R, G, B) 
		& $(0,\;0,\;0)$ \\
		背景光强度 
		& $0.05$ \\
		光源位置 (X, Y, Z)
		& $(0,\;0,\;-100)$ \\
		光源能量 
		& $10^7$ \\
		\bottomrule
	\end{tabular}
\end{table}
\noindent


表 \ref{tab:camera-param} 中展示了渲染时设置的相机内外参。其中分辨率为 $1920 \times 1200$，像素焦距约 $3003.413\,\mathrm{px}$，主点设在图像中心 $(960, 600)$。畸变系数说明了镜头的径向和切向畸变分布；相机外参则由位置 $(0,0,0)$ 和欧拉角 $(0^\circ,-180^\circ,0^\circ)$ 指定。

表 \ref{tab:env-param} 中的空间环境与光照参数包括全局背景颜色与强度，以模拟深空环境下的黑色背景和较低的环境光照；同时给出了场景中主要光源的位置和能量，将光源放置在 $Z$ 轴负方向 100 米处；光照能量设为$10^7$用以模拟照明。

通过上述参数设置，BlenderProc成功生成了一系列包含目标物体不同姿态和位置的高质量渲染图像。这些图像配合对应的6D位姿标签，构成了用于训练和评估6D位姿估计模型的理想数据集。生成的数据集效果图如图\ref{fig:rendered_datasets}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/rendered_datasets.png}
	\caption{BlenderProc渲染出的位姿数据集}
	\label{fig:rendered_datasets}
	\vspace{-3ex}
\end{figure}

\subsection{关键点的3D坐标提取}

由于仿真模型是由离散的多边形网格(Mesh)组成，每个网格由边与边围成的面构成，直接可用的关键点往往较为稀疏。为此，首先提取模型的整体网格框架，并将每条网格边的顶点收集得到初始点云。但该初始点云较为稀疏，尤其在局部镂空及曲率较大区域难以捕捉足够密集的形状细节。为解决此问题，可通过手动添加稠密几何表面补全，并将 Catmull-Clark 细分算法应用到原网格上，从而生成更为丰富的顶点集，以便选取关键点。图\ref{fig:model2pointcloud}演示了将仿真模型转化为基础点云的示意过程。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{Img/extract_point_cloud.png}
	\caption{仿真模型转点云示意图}
	\label{fig:model2pointcloud}
	\vspace{-3ex}
\end{figure}



Catmull-Clark细分算法由Edwin Catmull和Jim Clark于1978年提出，是计算机图形学中经典的细分曲面方法之一。其基本思想是：在原有网格基础上不断插入新顶点并更新已有顶点坐标，以获得更高分辨率、更平滑的网格。通常对于每个多边形面，会取该面的所有顶点坐标平均值生成一个新"面顶点"；对于每条边，则将其两端顶点坐标及邻接面的面顶点坐标进行平均，生成新的"边顶点"；对原有顶点本身，则结合邻域内的面顶点、相邻边中点以及原坐标，通过加权平均获取更新后的顶点位置，常见的更新公式可写作

\begin{equation}
	P_{\mathrm{new}} = \frac{F + 2R + (n-3)O}{n},
\end{equation}

其中 $F$ 表示该顶点周围所有相邻面的面顶点坐标平均值，$R$ 是相邻边中点坐标的平均值，$O$ 是原顶点坐标，$n$ 为此顶点的邻域面数。通过多次迭代上述插值与更新步骤，原有网格将细化为一个包含大量顶点的平滑网格。

在本研究中，通过在局部镂空或曲率突变区域手动补点，并将Catmull-Clark细分算法应用到初始稀疏网格上，使得整体网格顶点明显增多。将细分后网格的所有顶点导出后，即可得到更密集、更均匀的点云，有利于捕捉模型上较为复杂或细微的几何特征，也为后续计算目标的转动惯量和质心提供更贴近真实航天器的几何信息。对于在局部区域不希望过度平滑或出现边缘软化的问题，也可对该算法进行针对性的修正，例如在边缘处施加保持锐度的约束，或结合自适应细分技术保证局部精度。综合利用上述方法后，本文不仅能够在镂空区域和高曲率区获取更可靠的稠密点云数据，也为后续关键点提取及形变分析提供了更为丰富的几何信息。

\subsection{渲染数据集关键点的标注}

经过以上方法可得到较稠密的点云，再根据特征明显程度以及尽可能均匀分布的原则，分别在Nauka MLM与Starlink的稠密点云上直接选取了17个和12个关键点，如图\ref{fig:rendered_lableled_kpts}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/rendered_lableled_kpts.png}
	\caption{渲染数据集的关键点标注示例}
	\label{fig:rendered_lableled_kpts}
	\vspace{-3ex}
\end{figure}



\section{模型训练策略}

\subsection{数据增广}
为了预防模型在数据集上发生过拟合，同时也为了提升模型的泛化能力，这里对数据集进行了几何与色彩两种增广。增广效果如图\ref{fig:data_augmentation}所示，
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/data_augmentation.png}
	\caption{数据增广}
	\label{fig:data_augmentation}
	\vspace{-3ex}
\end{figure}

在色彩增广中，本文选择使用 HSV（色调、饱和度、亮度）色彩空间而非 RGB 色彩空间。HSV 色彩空间更符合人类对颜色的感知方式，能够独立调整色调、饱和度和亮度，而不会影响图像的结构信息。这有助于模型学习在不同光照和颜色条件下的鲁棒性。具体增广参数如下：

色调调整（\texttt{hsv\_h}: 0.015）对图像的色调进行微小调整，幅度为 0.015，引入颜色的微小变化；饱和度调整（\texttt{hsv\_s}: 0.7）调整图像的饱和度，范围为 0.7，增强或减弱颜色的纯度；亮度调整（\texttt{hsv\_v}: 0.4）调整图像的亮度，范围为 0.4，使模型适应不同的亮度条件。

几何增广通过改变图像的空间结构，增强模型对不同姿态和视角的适应性。具体增广方法和参数如表~\ref{tab:geometric_augmentation}所示：

\begin{table}[htbp]
	\centering
	\caption{几何增广方法与参数设置}
	\label{tab:geometric_augmentation}
	\begin{tabular}{lcp{8cm}}
		\toprule
		增广方法 & 参数值 & 作用描述 \\
		\midrule
		平移（translate） & 0.1 & 在水平和垂直方向上平移图像，幅度为图像尺寸的10\%，模拟目标位置的变化 \\
		缩放（scale） & 0.5 & 按比例缩放图像，缩放因子在0.5范围内变化，模拟不同的目标尺度 \\
		左右翻转（fliplr） & 0.5 & 以50\%的概率对图像进行左右翻转，增加数据的多样性 \\
		马赛克拼接（mosaic） & 1.0 & 将四张图像拼接成一张，模拟复杂背景和多目标场景 \\
		\bottomrule
	\end{tabular}
\end{table}
\subsection{优化器设置}
优化器在模型训练中起着关键作用，直接影响模型的收敛速度和效果。考虑到本文研究的数据集的训练样本高达4万多张图像，因此本文选择了 SGD 优化器。SGD 优化器是一种基于随机梯度下降的方法，通过对每个训练样本或小批量样本计算梯度并更新参数，能够较好地避免局部最优问题，并具有良好的泛化能力。此外，SGD 的实现简单且计算复杂度低，适用于大规模数据集。在结合合适的学习率调度和动量设置后，SGD 能够实现稳定且高效的模型收敛。因此，综合考虑模型训练的稳定性和泛化性能，本文最终选择了 SGD 优化器。 

\subsection{动量的应用}
动量（Momentum）优化方法在梯度下降中引入了物理学中的动量概念，其更新公式为：
\begin{equation}
	v_t = \beta v_{t-1} + (1 - \beta) \nabla L(\theta_{t-1})
\end{equation}
\begin{equation}
	\theta_t = \theta_{t-1} - \alpha v_t
\end{equation}
其中，$v_t$ 是第 $t$ 次迭代的动量项，$\beta$ 是动量超参数，取值为 0.937，$\nabla L(\theta_{t-1})$ 是损失函数关于参数的梯度，$\alpha$ 是学习率。动量项帮助优化器在参数更新时保留之前的更新方向，减少震荡，加速收敛。

\subsection{权重衰减}
权重衰减（Weight Decay）是一种正则化技术，通过在损失函数中添加 L2 范数惩罚项，防止模型过拟合。其损失函数形式为：
\begin{equation}
	L_{\text{total}} = L_{\text{original}} + \frac{\lambda}{2} \sum_{i} \theta_i^2
\end{equation}
其中，$L_{\text{original}}$ 为原始损失函数，$\lambda$ 是权重衰减系数，取值为 0.0005，$\theta_i$ 为模型参数。权重衰减通过抑制参数的过大增长，增强模型的泛化能力。

\subsection{学习率策略}
学习率是影响模型训练效果的关键超参数。本文采用了学习率预热和学习率衰减策略。学习率预热在训练初期使用较小的学习率，逐步增加到预设的初始学习率，防止训练初期的不稳定。预热策略的公式为：
\begin{equation}
	\alpha_t = \alpha_{\text{init}} \times \frac{t}{T_{\text{warmup}}}
\end{equation}
其中，$\alpha_t$ 为第 $t$ 次迭代的学习率，$\alpha_{\text{init}}$ 为预设的初始学习率（取值为 0.01），$T_{\text{warmup}}$ 为预热周期（取值为 3 个 epoch）。
在训练过程中，逐步降低学习率，有助于模型在收敛后期进行精细的参数调整。最终学习率为初始学习率的 1\%：
\begin{equation}
	\alpha_{\text{final}} = \alpha_{\text{init}} \times \texttt{lrf}
\end{equation}
其中，\texttt{lrf} 为学习率衰减因子，取值为 0.01。

\subsection{移动指数平均技术}
为提升模型的稳定性和泛化能力，采用了移动指数平均（Exponential Moving Average, EMA）技术。EMA 对模型参数进行指数加权平均，其更新公式为：
\begin{equation}
	\theta_{\text{EMA}}^t = \beta \theta_{\text{EMA}}^{t-1} + (1 - \beta) \theta^t
\end{equation}
其中，$\theta_{\text{EMA}}^t$ 为第 $t$ 次迭代的 EMA 参数，$\theta^t$ 为当前模型参数，$\beta$ 为衰减率，通常取值 0.999。EMA 减少了模型参数的波动，提升了模型在验证集和测试集上的表现。

\subsection{训练参数}
综合上述策略，模型训练的主要参数如表~\ref{tab:train_param}所示
\begin{table}[htbp]
	\centering
	\caption{训练参数}
	\label{tab:train_param}
	{%
		\zihao{5} % 局部将表格内字号设置为五号
		\renewcommand{\arraystretch}{1.2}% 调整行间距
		\begin{tabular}{cc}  % 两列均采用居中对齐
			\toprule[1.5pt]
			参数        & 设置 \\
			\midrule[1pt]
			训练周期    & 400 个 epoch \\
			批量大小    & 16 \\
			优化器      & SGD \\
			初始学习率  & 0.01 \\
			动量        & 0.937 \\
			权重衰减    & 0.0005 \\
			\bottomrule[1.5pt]
		\end{tabular}
	}
\end{table}

通过以上设计的训练策略和参数设置，模型能够有效收敛，预防过拟合，增强对多样化情况的鲁棒性。
\subsection{训练环境}
训练环境如表~\ref{tab:training_env}所示
% 第二个表格：训练环境介绍
\begin{table}[htbp]
	\centering
	\caption{训练环境介绍}
	\label{tab:training_env}
	{%
		\zihao{5} % 局部将表格内字号设置为五号
		\renewcommand{\arraystretch}{1.2}% 调整行间距
		\begin{tabular}{cc}  % 两列均采用居中对齐
			\toprule[1.5pt]
			系统配置   & 配置 \\
			\midrule[1pt]
			操作系统   & Rocky Linux 8.6 \\
			CPU        & Intel(R) Xeon(R) Platinum 8358P \\
			GPU        & NVIDIA A800 \\
			内存       & 64GB \\
			Python     & 3.9 \\
			PyTorch    & 2.2.1 \\
			CUDA       & 12.1 \\
			\bottomrule[1.5pt]
		\end{tabular}
	}
\end{table}
\section{实验分析}
为了有效验证相关模块和损失函数改进对于精度提升的有效性，接下来将进行一系列的消融实验，关键点检测的最终目的是得出目标的6D姿态，为了更有效地评估关键点检测模型对于最终结果的影响，本节的实验将把模型输出的关键点通过PnP解算出6D姿态，在6D姿态的评估指标上验证相关模块和损失函数的改进所带来的影响。
\subsection{评估指标}
为了有效衡量6D姿态估计的精度，本节实验采用了空间非合作目标6D姿态估计领域常见的指标，该指标由欧空局在SPEED/SPEED+ 数据集上提出，并在大量相关文献中采用。相关的指标定义如下所示。
\begin{itemize}
	\item $err_{\text{t}}$: 真值与预测位置之间的归一化相对误差，表示位置估计的相对精度。该分数无量纲，提供了对位置误差的尺度无关度量。定义如下：
	\begin{equation}
		err_{t,i} = \frac{\| r_{\text{true},i} - r_{\text{pred},i} \|_2}{\| r_{\text{true},i} \|_2}.
	\end{equation}
	
	\begin{equation}
		err_t = \frac{1}{N} \sum\limits_{i=1}^{N} err_{t,i}.
	\end{equation}
	
	其中 $r_{\text{true},i}$ 和 $r_{\text{pred},i}$ 分别为第 $i$ 个样本的真值和预测位置向量，$N$ 表示样本总数。
	
	\item $err_{\text{T}}$: 表示真值与预测位置之间的绝对位置误差（单位：米）。该误差度量了预测位置与真实位置之间的欧几里得距离。
	\begin{equation}
		err_T = \frac{1}{N}\sum\limits_{i=1}^{N} \| r_{\text{true},i} - r_{\text{pred},i} \|_2.
	\end{equation}
	
	其中 $r_{\text{true},i}$ 和 $r_{\text{pred},i}$ 分别为第 $i$ 个样本的真值和预测位置向量，$N$ 表示样本总数。
	
	\item $err_{\text{ort}}^{\text{rad}}$: 通过比较预测姿态和真实姿态的相对方向来度量姿态误差，单位为弧度。该度量强调相对度量相比绝对度量的优势。定义如下：
	\begin{equation}
		err_{\text{ort, i}}^{\text{rad}} =  2 \cdot \arccos \left( \left| \langle q_{\text{pred},i}, q_{\text{true},i} \rangle \right| \right).
	\end{equation}
	
	\begin{equation}
		err_{\text{ort}}^{\text{rad}} = \frac{1}{N}\sum\limits_{i=1}^N err_{\text{ort, i}}^{\text{rad}}.
	\end{equation}
	
	其中 $q_{\text{pred},i}$ 和 $q_{\text{true},i}$ 分别为第 $i$ 个样本的预测与真实姿态四元数，$N$ 表示样本总数，$\langle \cdot, \cdot \rangle$ 表示四元数的内积。
	
	\item $err_{\text{ort}}^{\circ}$: 以度数形式表示的 $err_{\text{ort}}$。定义如下：
	\begin{equation}
		err_{\text{ort, i}}^{\circ} =  \frac{180}{\pi}^{\circ} err_{\text{ort, i}}^{\text{rad}}.
	\end{equation}
	\begin{equation}
		err_{\text{ort}}^{\circ} = \frac{1}{N}\sum\limits_{i=1}^N err_{\text{ort, i}}^{\circ}. 
	\end{equation}
	
	\item $score_{\text{pst}}$: SPEED 数据集中的位置评分，数值越低表示精度越高。定义如下：
	\begin{equation}
		score_{\text{pst}} = err_{\text{t}}.
	\end{equation}
	
	\item $score_{\text{ort}}$: SPEED 数据集中的姿态评分，数值越低表示精度越高。定义如下：
	\begin{equation}
		score_{\text{ort}} = err_{\text{ort}}^{\text{rad}}.
	\end{equation}
	
	\item $score$: SPEED 数据集的总评分，将位置和姿态的结果进行综合，作为 6D 姿态估计的整体评价指标。定义如下：
	\begin{equation}
		score = score_{\text{ort}} + score_{\text{pst}}.
	\end{equation}
	该综合评分在同时考虑位置和姿态准确度的情况下提供单一指标来评估整体表现，得分越低说明精度越高。
	
	\item $score_{\text{pst}}^+$: SPEED+ 数据集中的位置评分。定义如下：
	\begin{equation}
		score_{\text{pst, i}}^+ = 
		\begin{cases}
			0, & \text{if } err_{\text{t, i}} < 0.002173 \\
			err_{\text{t, i}}, & \text{otherwise}
		\end{cases}.
	\end{equation}
	
	\begin{equation}
		score_{\text{pst}}^+ = \frac{1}{N}\sum\limits_{i=1}^N score_{\text{pst, i}}^+.
	\end{equation}
	
	\item $score_{\text{ort}}^+$: SPEED+ 数据集中的姿态评分。定义如下：
	\begin{equation}
		score_{\text{ort, i}}^+ = 
		\begin{cases}
			0, & \text{if } err_{\text{ort, i}}^{\circ} < 0.169^\circ \\
			err_{\text{ort, i}}^{\text{rad}}, & \text{otherwise}
		\end{cases}.
	\end{equation}
	\begin{equation}
		score_{\text{ort}}^+ = \frac{1}{N}\sum\limits_{i=1}^N score_{\text{ort, i}}^+.
	\end{equation}
	
	\item $score^+$: SPEED+ 数据集官方评价中的综合评分，基于位置和姿态的单独评估，分数越低表示精度越高。定义如下：
	\begin{equation}
		score^+ = \frac{1}{N}\sum\limits_{i=1}^N(score_{\text{ort, i}}^+ + score_{\text{pst, i}}^+).
	\end{equation}
	
\end{itemize}




\subsection{EfficientViT消融实验}
本文进行了消融实验，使用三个数据集和三个度量指标 $score_{\text{pst}}^+$、$score_{\text{ort}}^+$ 以及 $score^+$（分别表示旋转、平移和总体误差），对比了 EfficientViT 与 YOLOv8n-Pose 原始骨干网络的性能。实验数据如表~\ref{tab:EfficientViTAblation} 所示。实验结果表明，虽然 EfficientViT 的参数量增加了约 30\%，但在大多数数据集上，其精度均得到了显著提升。

对于 Nauka MLM 数据集，$score_{\text{pst}}^+$ 的精度提升了 18.60\%，$score_{\text{ort}}^+$ 提高了 16.48\%，而 $score^+$ 则提升了 16.92\%。对于 Starlink 数据集，提升效果更加明显，$score_{\text{pst}}^+$、$score_{\text{ort}}^+$ 和 $score^+$ 分别提高了 36.51\%、24.60\% 和 28.78\%。SPEED+ 数据集显示了最显著的提升，$score_{\text{pst}}^+$、$score_{\text{ort}}^+$ 和 $score^+$ 分别提升了 39.63\%、39.68\% 和 39.67\%。

尽管大多数样本表现出较高的估计精度，但选择了一些误差较为明显的案例进行可视化，以更好地展示消融实验的影响。可视化图像中，白色实线框与红绿蓝实线箭头为6D姿态的真值，白色虚线框与红绿蓝虚线箭头为6D姿态的预测值。如图~\ref{fig:backbone_abliation} 所示，在所有三个数据集中，采用 EfficientViT 后预测得到的红色姿态框和姿态箭头与真实值更加吻合。在一些样本中，例如 SPEED+ 数据集中的第四个案例，YOLOv8 骨干网络在 6D 姿态估计中表现出明显的向左旋转偏差；而采用 EfficientViT 的骨干网络有效缓解了这一问题，其姿态估计结果与真实值高度一致。对于原本存在大量样本显示出明显平移和旋转偏差的 Starlink 数据集，在引入 EfficientViT 后得到了显著改善。这些结果突显了 EfficientViT 中级联群注意力机制在提升姿态估计精度方面的有效性。特别是 SPEED+ 数据集，展现了最为显著的提升。这主要归因于该数据集所面临的独特挑战，不仅包括多样化的目标姿态变化，还存在地球背景干扰。传统的 YOLOv8 骨干网络可能难以在这些复杂条件下集中于目标区域的关键点提取。相比之下，引入的 EfficientViT 利用其自注意力机制，即便在存在背景干扰的情况下，也展现出更强的聚焦于目标区域关键点提取的能力。这些发现凸显了 EfficientViT 在处理复杂场景时的鲁棒性，以及其在显著提升非合作空间物体 6D 姿态估计精度方面的潜力。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/backbone_abliation.pdf}
	\caption{骨干网络消融实验}
	\label{fig:backbone_abliation}
	\vspace{-3ex}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{Img/pin_hole_model.pdf}
	\caption{小孔相机模型}
	\label{fig:pin_hole_model}
	\vspace{-3ex}
\end{figure}
\begin{table}[htbp]
	\centering
	\caption{在SPEED+、Nauka MLM、Starlink合成数据集上的EfficientViT消融实验}
	\label{tab:EfficientViTAblation}
	{%
		\zihao{5} % 将表内字号设为五号
		\begin{tabular}{c c c c c c} 
			\toprule[1.5pt]
			数据集 & 主干网络 & 参数量 & $score_{\text{ort}}^+$ & $score_{\text{pst}}^+$ & $score^+$ \\
			\midrule[1pt]
			\multirow{2}{*}{SPEED+}
			& YOLOv8n      & 3.2539 & 0.0382            & 0.0164            & 0.0546 \\
			& EfficientViT & 4.2544 & \textbf{0.0225}   & \textbf{0.0097}   & \textbf{0.0322} \\
			\midrule[1pt]
			\multirow{2}{*}{Nauka MLM}
			& YOLOv8n      & 3.2955 & 0.0272            & 0.0130            & 0.0402 \\
			& EfficientViT & 4.2959 & \textbf{0.0228}   & \textbf{0.0104}   & \textbf{0.0332} \\
			\midrule[1pt]
			\multirow{2}{*}{Starlink}
			& YOLOv8n      & 3.1956 & 0.0446            & 0.0240            & 0.0686 \\
			& EfficientViT & 4.1960 & \textbf{0.0326}   & \textbf{0.0144}   & \textbf{0.0469} \\
			\bottomrule[1.5pt]
		\end{tabular}
	}
\end{table}

考虑到基于视觉Transformer的EfficientViT擅长捕捉更全局的特征，并且对细粒度细节的依赖较低，本章还额外渲染了明亮和暗光条件下的Starlink姿态图像以进行进一步实验。表~\ref{tab:EfficientViT Abliation_light}展示了在这些不同光照条件下的消融实验结果。

如图~\ref{fig:bright_scene_comp}所示，展示了在明亮条件下进行骨干消融的姿态估计结果；类似地，图~\ref{fig:dark_scene_comp}展示了暗光条件下的结果。在这两种场景下，EfficientViT骨干网络的预测6D姿态与真实值的对齐程度均优于YOLOv8，这证明了EfficientViT在不同光照条件下具有更强的鲁棒性。

\begin{table}[!htbp]
	\centering
	\caption{在明暗场景下的骨干网络消融实验}
	\label{tab:EfficientViT Abliation_light}
	{%
		\zihao{5} % 将表内文字字号设为五号
		\setlength{\tabcolsep}{6.5mm}{
			\begin{tabular}{ccccc}
				\toprule[1.5pt]
				数据集 & 骨干网络 & $score_{\text{pst}}^+$ & $score_{\text{ort}}^+$ & $score^+$ \\
				\midrule[1pt]
				\multirow{2}{*}{明亮场景}
				& EfficientViT & \textbf{0.1015} & \textbf{0.5452} & \textbf{0.6467} \\
				& YOLOv8       & 0.1693          & 0.7631          & 0.9324 \\
				\midrule[1pt]
				\multirow{2}{*}{暗光场景}
				& EfficientViT & \textbf{0.0256} & \textbf{0.1822} & \textbf{0.2078} \\
				& YOLOv8       & 0.0616          & 0.2165          & 0.2781 \\
				\bottomrule[1.5pt]
			\end{tabular}
		}
	}
\end{table}


\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/bright_scene_for _backbone_aliation.pdf}
	\caption{亮场景下的骨干网络消融实验}
	\label{fig:bright_scene_comp}
	\vspace{-3ex}
\end{figure}



\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/dark_scene_for _backbone_aliation.pdf}
	\caption{暗场景下的骨干网络消融实验}
	\label{fig:dark_scene_comp}
	\vspace{-3ex}
\end{figure}



\vspace{12pt}
\subsection{TANeck消融实验}

本文对 TANeck 进行了消融实验，其方法与 EfficientViT 的消融实验类似，并使用了相同的三个数据集。实验结果如表~\ref{tab:TANeck_abliation} 所示。应用 TANeck 后，各项指标均显著提升。对于 SPEED+ 数据集，$score_{\text{pst}}^+$、$score_{\text{ort}}^+$ 以及 $score^+$ 分别提高了 17.07\%、10.85\% 和 12.55\%。对于 Nauka MLM 数据集，这些指标分别提高了 15.50\%、12.45\% 和 13.43\%，而在 Starlink 数据集中，这些指标的提升更为明显，分别达到了 36.93\%、27.74\% 和 30.95\%。Starlink相对于其他的 SPEED+/SPEED 中的 Tango 卫星以及 Nauka MLM 航天器，其表面的纹理这种细粒度特征更难识别，对于太阳帆板的纹理模式显得较为重复，而标注的关键点又大多位于其表面。对于位于其本体部分的地方，由于其材质的原因，使其的反射率较低，通常处于黑色，这也进一步加剧纹理等细粒度特征识别的难度。传统的YOLOv8在这类目标的关键点检测上存在困难。而引入Triplet Attention构造的TANeck在细粒度特征提取能力上得到了明显的提升，因此才使得其在检测这类特殊目标时，提升幅度明显增大。


\begin{table}[!htbp]
	\centering
	\caption{TANeck 消融实验}
	\label{tab:TANeck_abliation}
	{%
		\zihao{5} % 将表格内文字字号设为五号
		\setlength{\tabcolsep}{6.5mm}{
			\begin{tabular}{ccccc}
				\toprule[1.5pt]
				数据集 & Neck & $score_{\text{pst}}^+$ & $score_{\text{ort}}^+$ & $score^+$ \\
				\midrule[1pt]
				\multirow{2}{*}{SPEED+}
				& TANeck  & \textbf{0.0136} & \textbf{0.0337} & \textbf{0.0474} \\
				& YOLOv8  & 0.0164          & 0.0378          & 0.0542          \\
				\midrule[1pt]
				\multirow{2}{*}{Nauka MLM}
				& TANeck  & \textbf{0.0109} & \textbf{0.0239} & \textbf{0.0348} \\
				& YOLOv8  & 0.0129          & 0.0273          & 0.0402          \\
				\midrule[1pt]
				\multirow{2}{*}{Starlink}
				& TANeck  & \textbf{0.0152} & \textbf{0.0323} & \textbf{0.0475} \\
				& YOLOv8  & 0.0241          & 0.0447          & 0.0688          \\
				\bottomrule[1.5pt]
			\end{tabular}
		}
	}
\end{table}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/TANeck_Abliation.pdf}
	\caption{TANeck消融实验}
	\label{fig:TANeck_abliation}
	\vspace{-3ex}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/A3DKS.pdf}
	\caption{目标不同位姿投影示意图}
	\label{fig:projection_area_compare}
	\vspace{-3ex}
\end{figure}

\subsection{A3DKS消融实验} \label{A3DKS_Ablation_subsection}

本文将 A3DKS 损失函数与 YOLOv8-pose 中使用的 OKS 损失函数进行比较，以验证 A3DKS 的有效性。结果如表~\ref{tab:OKSvsA3DKS} 所示：使用 A3DKS 后，所有数据集上的 $score^+$ 与 $score_{\text{ort}}^+$ 均有所提升。值得注意的是，Nauka MLM 和 Starlink 上的提升更为显著，分别达到了 32.97\% 和 21.70\%，而 SPEED+ 的提升仅为 10.58\%。这主要是因为 Starlink 和 Nauka MLM 的目标形状较为细长，即使在目标位置保持不变、仅姿态发生变化时，其二维透视投影也会出现更明显的变化，而这正是 A3DKS 在平衡关键点 3D 尺度误差敏感性方面所能发挥优势的场景。如图~\ref{fig:projection_area_compare} 所示，当 Starlink 的太阳能帆板逐渐转至接近垂直于投影平面时，其投影面积显著减小，同时关键点的纵深分布明显增大。这加剧了投影点在 3D 尺度上的不平衡，使纵深较大的关键点与纵深较小的关键点在相同量级的像素噪声下呈现出更加不一致的 3D 偏差。

如图~\ref{fig:A3DKS} 所示，A3DKS 损失在三种不同目标上的作用均使其 6D 姿态估计相较于 OKS 有一定程度的误差降低。通过分析旋转精度指标 $score_{\text{ort}}^+$ 和位置精度指标 $score_{\text{pst}}^+$，可以看出旋转估计精度得到了显著提升，但位置精度略有下降：具体来说，SPEED+ 的 $score_{\text{pst}}^+$ 下降了 9.76\%，而 Nauka MLM 和 Starlink 分别下降了 6.20\% 和 1.24\%。这通常与关键点的误差敏感性以及旋转带来的"杠杆效应"相关：纵深分布越深的关键点在 3D 空间中离"支点"越远，因而图像平面上的微小像素偏差更容易引发另一端的明显变化。虽然 A3DKS 能在一定程度上抑制这类误差，但也有可能导致关键点在某个方向上的集中偏移，从而增加平移误差。尽管位置精度指标略有下降，综合得分 $score^+$ 仍然有所提升，表明整体而言 A3DKS 具备更优的综合性能。



\begin{table*}[htbp] 
	\centering
	\caption{损失函数消融实验}
	\label{tab:OKSvsA3DKS}
	{%
		\zihao{5} % 将表格内文字字号设为五号
		\setlength{\tabcolsep}{6mm}{
			\begin{tabular}{ccccc}
				\toprule[1.5pt]
				数据集 & 损失函数 & $score_{\text{ort}}^+$ & $score_{\text{pst}}^+$ & $score^+$ \\
				\midrule[1pt]
				\multirow{2}{*}{SPEED+}
				& OKS   & 0.0382          & \textbf{0.0164} & 0.0546 \\
				& A3DKS & \textbf{0.0339} & 0.0184          & \textbf{0.0523} \\
				\midrule[1pt]
				\multirow{2}{*}{Nauka MLM}
				& OKS   & 0.0272          & \textbf{0.0130} & 0.0402 \\
				& A3DKS & \textbf{0.0183} & 0.0137          & \textbf{0.0320} \\
				\midrule[1pt]
				\multirow{2}{*}{Starlink}
				& OKS   & 0.0446          & \textbf{0.0240} & 0.0686 \\
				& A3DKS & \textbf{0.0349} & 0.0243          & \textbf{0.0591} \\
				\bottomrule[1.5pt]
			\end{tabular}
		}
	}
\end{table*}




\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{Img/A3DKS_abliation.pdf}
	\caption{A3DKS消融实验}
	\label{fig:A3DKS}
	\vspace{-3ex}
\end{figure}









\subsection{本章小结}
本章围绕空间非合作目标关键点检测的研究背景，首先回顾了传统基于特征点（SIFT、FAST、ORB 等）以及基于深度学习（坐标回归和热图预测）的关键点检测方法，并对在人形关键点检测或航天器关键点检测中常用的经典网络（如 YOLO Pose、Stacked Hourglass、HRNet 等）进行了概述。然后介绍了在航天器位姿估计中常用的 SPEED/SPEED+ 数据集，并阐述了如何通过多视图几何在缺乏关键点标注的数据集中自动生成关键点标注。为进一步提升模型的鲁棒性和泛化能力，本章还介绍了利用 BlenderProc 对 Nauka MLM 和 Starlink 两种不同形状的航天器模型进行渲染，构建了带有关键点标注的 6D 位姿数据集，并阐述了通过 Catmull-Clark 细分算法在仿真模型网格上提取稠密点云和选择关键点的过程。

在网络模型设计部分，本章提出了在 YOLOv8 的基础上，利用轻量化的Transformer：EfficientViT作为  骨干网络（EfficientViT）和的 TANeck 结构。EfficientViT 侧重于全局注意力，帮助模型捕捉更丰富的全局及区域上下文，；而TANeck则通在YOLOv8-pose 架构的Neck引入Triplet Attention作为通路3，有效的提升了网络对细粒度特征的关注度。同时，本章还提出了结合目标深度信息的 A3DKS损失函数，以更好地刻画航天器的空间深度特性。

针对上述方法，本章在 SPEED+、Nauka MLM 以及 Starlink 三个数据集上进行了详细的消融实验。实验结果表明，EfficientViT 骨干网络的引入使得YOLOv8的全局特征提取能力得到增强，使其在光照过强，过暗或者过暗等局部细粒度特征弱化甚至消失的情况下依靠全局特征注意力增强实现了更高精度的检测，在这三类数据集上均带来了显著的 6D 姿态估计精度提升。而TANeck的引入使得YOLOv8的局部细粒度特征提取的能力得到了增强，使其在空间非合作目标弱纹理的情况下，明显地提升了关键点的检测精度。在空间非合作目标在空间中倾向于纵深姿态的时候，关键点由于三维投影的特性，使得较远距离的关键点对于关键点预测噪声更为敏感。A3DKS因其考虑关键点的三维深度影响，可显著改进对旋转姿态的估计，且对平移估计的精度也有一定帮助。然而，对某些目标（如 SPEED+ 中的 Tango），A3DKS 在部分场景中对平移精度造成了小幅影响。总体而言，A3DKS 尤其适合于深度维度变化幅度较大的目标（如 Nauka MLM、Starlink），在此类场景中可显著提升旋转精度。

综合来看，本章提出的基于注意力机制的关键点检测方法，在多个不同类型的非合作目标上均取得了较大幅度的精度的提升，纹理弱化、光照极端，关键点纵深分布较大等具有挑战性的场景中展现出了更高的精度与鲁棒性，为后续空间非合作目标 6D 姿态的解算的估计提供了可靠的输入。
